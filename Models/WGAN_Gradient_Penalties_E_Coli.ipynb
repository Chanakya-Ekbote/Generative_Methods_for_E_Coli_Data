{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# link :https://archive.ics.uci.edu/ml/datasets/ecoli\n",
    "import numpy as np \n",
    "import csv\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open('ecoli.csv', 'rt')\n",
    "reader = csv.reader(raw_data, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "data = list(reader)\n",
    "data = np.array(data)\n",
    "data = data[:,2: -2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29 0.48 0.5  0.56 0.24]\n",
      " [0.4  0.48 0.5  0.54 0.35]\n",
      " [0.4  0.48 0.5  0.49 0.37]\n",
      " ...\n",
      " [0.6  0.48 0.5  0.44 0.39]\n",
      " [0.61 0.48 0.5  0.42 0.42]\n",
      " [0.74 0.48 0.5  0.31 0.53]]\n",
      "(336, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "- \n",
    "    Sequences are made into a square image of closest binary number to the the ceiling of the root of the length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = np.shape(data)\n",
    "sqrt_col = int(np.ceil(np.sqrt(columns)))\n",
    "processed_data = np.zeros([rows, 3, 3])\n",
    "data_ = np.zeros([rows, 3*3])\n",
    "\n",
    "for i in range(rows):\n",
    "#     data[i] = (data[i] - np.mean(data[i]))\n",
    "    data_[i] = np.append(data[i], np.zeros(3*3 - columns))\n",
    "\n",
    "    \n",
    "for i in range(rows):\n",
    "    for j in range(sqrt_col):\n",
    "            processed_data[i,j,:] = data_[i, j * 3 : (j+1)*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29, 0.48, 0.5 ],\n",
       "       [0.56, 0.24, 0.  ],\n",
       "       [0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(input_shape):\n",
    "    \n",
    "    Input_1= Input(shape = input_shape)\n",
    "    \n",
    "    model = Conv2D(3, (3,3), activation='relu', input_shape=input_shape, padding ='same')(Input_1)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(4, (4,4), activation='relu', input_shape=input_shape, padding ='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(4, (4,4), activation='relu', input_shape=input_shape, padding ='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(1, (4,4), activation='tanh', input_shape=input_shape, padding ='same')(model)\n",
    "    output = BatchNormalization()(model)\n",
    "    \n",
    "    model = Model(inputs=Input_1, outputs = output, name = 'generator')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen = gen_model((3,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 3, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 3)           30        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 4)           196       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 3, 4)           16        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 4)           260       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 3, 4)           16        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 1)           65        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 1)           4         \n",
      "=================================================================\n",
      "Total params: 599\n",
      "Trainable params: 575\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_model(input_shape):\n",
    "    \n",
    "    Input_1= Input(shape = input_shape)\n",
    "    \n",
    "    model = Conv2D(3, (4,4), activation='relu', input_shape=input_shape, padding ='same')(Input_1)\n",
    "    model = BatchNormalization()(model)\n",
    "\n",
    "    model = Conv2D(3, (2,2), activation='relu', input_shape=input_shape, padding ='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(3, (2,2), activation='relu', input_shape=input_shape, padding ='same')(model)\n",
    "    model = BatchNormalization()(model)\n",
    "\n",
    "    model = Flatten()(model)\n",
    "    \n",
    "    output = Dense(1, activation = 'sigmoid')(model)\n",
    "    \n",
    "    model = Model(inputs=Input_1, outputs = output, name = 'discriminator')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = discrim_model((3,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 3, 3, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 3)           51        \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 3)           39        \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 3)           39        \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 175\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discrim.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the WGAN with Gradient Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_WGANgp(generator, discriminator, z_shape, img_shape, penaltyLambda):\n",
    "    #### model\n",
    "    # generator image(fake image)\n",
    "    z = Input(shape=z_shape)\n",
    "    f_img = generator(z)\n",
    "    f_out = discriminator(f_img)\n",
    "    # real image\n",
    "    r_img = Input(shape=img_shape)\n",
    "    r_out = discriminator(r_img)\n",
    "    # average image\n",
    "    epsilon = K.placeholder(shape=(None,1,1,1))\n",
    "    a_img = Input(shape=(img_shape),\n",
    "                  tensor = epsilon * r_img + (1-epsilon) * f_img)\n",
    "    a_out = discriminator(a_img)\n",
    "\n",
    "    #### loss\n",
    "    # original critic(discriminator) loss\n",
    "    r_loss = K.mean(r_out)\n",
    "    f_loss = K.mean(f_out)\n",
    "    # gradient penalty  <this is point of WGAN-gp>\n",
    "    grad_mixed = K.gradients(a_out, [a_img])[0]\n",
    "    norm_grad_mixed = K.sqrt(K.sum(K.square(grad_mixed), axis=[1,2,3]))\n",
    "    grad_penalty = K.mean(K.square(norm_grad_mixed -1))\n",
    "    penalty = penaltyLambda * grad_penalty\n",
    "    # d loss\n",
    "    d_loss = f_loss - r_loss + penalty\n",
    "    \n",
    "    #### discriminator update function\n",
    "    d_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n",
    "                get_updates(discriminator.trainable_weights,[],d_loss)\n",
    "    d_train = K.function([r_img, z, epsilon],\n",
    "                         [r_loss, f_loss, penalty, d_loss],\n",
    "                         d_updates)\n",
    "    \n",
    "    #### generator update function\n",
    "    g_loss = -1. * f_loss\n",
    "    g_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n",
    "                get_updates(generator.trainable_weights,[],g_loss)\n",
    "    g_train = K.function([z], [g_loss], g_updates)\n",
    "\n",
    "    return g_train, d_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\chanakya\\markets in smartgrids\\config\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_shape = (3,3,1)\n",
    "discrim_shape = (3,3,1)\n",
    "lamda = 0.05 # Hyperparameter\n",
    "# generator Model\n",
    "generator = gen_model(gen_shape)\n",
    "# discriminator Model\n",
    "discriminator = discrim_model(discrim_shape)\n",
    "# WGAN-gp Training Model\n",
    "G_train, D_train = build_WGANgp(generator, discriminator, gen_shape, discrim_shape, lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:1 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:2 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:3 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:4 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:5 | d_loss:0.050 | g_loss:-0.499\n",
      "iteration:6 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:7 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:8 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:9 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:10 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:11 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:12 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:13 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:14 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:15 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:16 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:17 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:18 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:19 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:20 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:21 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:22 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:23 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:24 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:25 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:26 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:27 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:28 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:29 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:30 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:31 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:32 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:33 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:34 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:35 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:36 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:37 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:38 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:39 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:40 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:41 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:42 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:43 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:44 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:45 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:46 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:47 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:48 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:49 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:50 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:51 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:52 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:53 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:54 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:55 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:56 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:57 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:58 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:59 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:60 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:61 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:62 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:63 | d_loss:0.047 | g_loss:-0.499\n",
      "iteration:64 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:65 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:66 | d_loss:0.046 | g_loss:-0.500\n",
      "iteration:67 | d_loss:0.047 | g_loss:-0.499\n",
      "iteration:68 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:69 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:70 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:71 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:72 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:73 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:74 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:75 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:76 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:77 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:78 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:79 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:80 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:81 | d_loss:0.046 | g_loss:-0.499\n",
      "iteration:82 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:83 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:84 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:85 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:86 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:87 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:88 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:89 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:90 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:91 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:92 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:93 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:94 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:95 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:96 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:97 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:98 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:99 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:100 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:101 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:102 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:103 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:104 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:105 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:106 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:107 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:108 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:109 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:110 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:111 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:112 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:113 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:114 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:115 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:116 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:117 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:118 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:119 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:120 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:121 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:122 | d_loss:0.044 | g_loss:-0.498\n",
      "iteration:123 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:124 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:125 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:126 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:127 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:128 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:129 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:130 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:131 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:132 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:133 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:134 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:135 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:136 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:137 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:138 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:139 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:140 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:141 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:142 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:143 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:144 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:145 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:146 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:147 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:148 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:149 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:150 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:151 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:152 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:153 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:154 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:155 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:156 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:157 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:158 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:159 | d_loss:0.042 | g_loss:-0.497\n",
      "iteration:160 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:161 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:162 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:163 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:164 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:165 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:166 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:167 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:168 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:169 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:170 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:171 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:172 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:173 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:174 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:175 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:176 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:177 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:178 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:179 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:180 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:181 | d_loss:0.041 | g_loss:-0.497\n",
      "iteration:182 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:183 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:184 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:185 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:186 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:187 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:188 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:189 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:190 | d_loss:0.040 | g_loss:-0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:191 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:192 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:193 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:194 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:195 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:196 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:197 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:198 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:199 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:200 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:201 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:202 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:203 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:204 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:205 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:206 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:207 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:208 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:209 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:210 | d_loss:0.040 | g_loss:-0.496\n",
      "iteration:211 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:212 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:213 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:214 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:215 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:216 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:217 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:218 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:219 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:220 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:221 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:222 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:223 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:224 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:225 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:226 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:227 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:228 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:229 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:230 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:231 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:232 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:233 | d_loss:0.039 | g_loss:-0.496\n",
      "iteration:234 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:235 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:236 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:237 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:238 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:239 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:240 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:241 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:242 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:243 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:244 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:245 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:246 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:247 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:248 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:249 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:250 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:251 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:252 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:253 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:254 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:255 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:256 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:257 | d_loss:0.038 | g_loss:-0.495\n",
      "iteration:258 | d_loss:0.038 | g_loss:-0.495\n",
      "iteration:259 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:260 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:261 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:262 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:263 | d_loss:0.038 | g_loss:-0.495\n",
      "iteration:264 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:265 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:266 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:267 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:268 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:269 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:270 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:271 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:272 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:273 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:274 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:275 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:276 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:277 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:278 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:279 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:280 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:281 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:282 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:283 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:284 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:285 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:286 | d_loss:0.037 | g_loss:-0.495\n",
      "iteration:287 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:288 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:289 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:290 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:291 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:292 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:293 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:294 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:295 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:296 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:297 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:298 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:299 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:300 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:301 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:302 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:303 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:304 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:305 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:306 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:307 | d_loss:0.036 | g_loss:-0.494\n",
      "iteration:308 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:309 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:310 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:311 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:312 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:313 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:314 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:315 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:316 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:317 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:318 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:319 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:320 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:321 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:322 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:323 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:324 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:325 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:326 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:327 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:328 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:329 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:330 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:331 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:332 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:333 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:334 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:335 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:336 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:337 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:338 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:339 | d_loss:0.034 | g_loss:-0.493\n",
      "iteration:340 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:341 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:342 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:343 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:344 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:345 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:346 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:347 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:348 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:349 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:350 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:351 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:352 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:353 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:354 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:355 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:356 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:357 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:358 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:359 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:360 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:361 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:362 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:363 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:364 | d_loss:0.032 | g_loss:-0.492\n",
      "iteration:365 | d_loss:0.032 | g_loss:-0.492\n",
      "iteration:366 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:367 | d_loss:0.032 | g_loss:-0.492\n",
      "iteration:368 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:369 | d_loss:0.032 | g_loss:-0.492\n",
      "iteration:370 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:371 | d_loss:0.032 | g_loss:-0.492\n",
      "iteration:372 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:373 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:374 | d_loss:0.032 | g_loss:-0.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:375 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:376 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:377 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:378 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:379 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:380 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:381 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:382 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:383 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:384 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:385 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:386 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:387 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:388 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:389 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:390 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:391 | d_loss:0.031 | g_loss:-0.492\n",
      "iteration:392 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:393 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:394 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:395 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:396 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:397 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:398 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:399 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:400 | d_loss:0.030 | g_loss:-0.492\n",
      "iteration:401 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:402 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:403 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:404 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:405 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:406 | d_loss:0.030 | g_loss:-0.491\n",
      "iteration:407 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:408 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:409 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:410 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:411 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:412 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:413 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:414 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:415 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:416 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:417 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:418 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:419 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:420 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:421 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:422 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:423 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:424 | d_loss:0.029 | g_loss:-0.491\n",
      "iteration:425 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:426 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:427 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:428 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:429 | d_loss:0.028 | g_loss:-0.491\n",
      "iteration:430 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:431 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:432 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:433 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:434 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:435 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:436 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:437 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:438 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:439 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:440 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:441 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:442 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:443 | d_loss:0.028 | g_loss:-0.490\n",
      "iteration:444 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:445 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:446 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:447 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:448 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:449 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:450 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:451 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:452 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:453 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:454 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:455 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:456 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:457 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:458 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:459 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:460 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:461 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:462 | d_loss:0.027 | g_loss:-0.489\n",
      "iteration:463 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:464 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:465 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:466 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:467 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:468 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:469 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:470 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:471 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:472 | d_loss:0.027 | g_loss:-0.489\n",
      "iteration:473 | d_loss:0.025 | g_loss:-0.489\n",
      "iteration:474 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:475 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:476 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:477 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:478 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:479 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:480 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:481 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:482 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:483 | d_loss:0.025 | g_loss:-0.490\n",
      "iteration:484 | d_loss:0.026 | g_loss:-0.489\n",
      "iteration:485 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:486 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:487 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:488 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:489 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:490 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:491 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:492 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:493 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:494 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:495 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:496 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:497 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:498 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:499 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:500 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:501 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:502 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:503 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:504 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:505 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:506 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:507 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:508 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:509 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:510 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:511 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:512 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:513 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:514 | d_loss:0.027 | g_loss:-0.490\n",
      "iteration:515 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:516 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:517 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:518 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:519 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:520 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:521 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:522 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:523 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:524 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:525 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:526 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:527 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:528 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:529 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:530 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:531 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:532 | d_loss:0.026 | g_loss:-0.490\n",
      "iteration:533 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:534 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:535 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:536 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:537 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:538 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:539 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:540 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:541 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:542 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:543 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:544 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:545 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:546 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:547 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:548 | d_loss:0.027 | g_loss:-0.491\n",
      "iteration:549 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:550 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:551 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:552 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:553 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:554 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:555 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:556 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:557 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:558 | d_loss:0.027 | g_loss:-0.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:559 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:560 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:561 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:562 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:563 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:564 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:565 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:566 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:567 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:568 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:569 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:570 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:571 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:572 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:573 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:574 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:575 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:576 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:577 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:578 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:579 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:580 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:581 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:582 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:583 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:584 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:585 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:586 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:587 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:588 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:589 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:590 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:591 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:592 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:593 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:594 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:595 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:596 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:597 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:598 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:599 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:600 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:601 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:602 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:603 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:604 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:605 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:606 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:607 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:608 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:609 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:610 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:611 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:612 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:613 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:614 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:615 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:616 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:617 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:618 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:619 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:620 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:621 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:622 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:623 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:624 | d_loss:0.025 | g_loss:-0.492\n",
      "iteration:625 | d_loss:0.024 | g_loss:-0.492\n",
      "iteration:626 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:627 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:628 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:629 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:630 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:631 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:632 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:633 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:634 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:635 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:636 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:637 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:638 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:639 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:640 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:641 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:642 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:643 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:644 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:645 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:646 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:647 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:648 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:649 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:650 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:651 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:652 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:653 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:654 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:655 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:656 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:657 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:658 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:659 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:660 | d_loss:0.023 | g_loss:-0.491\n",
      "iteration:661 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:662 | d_loss:0.023 | g_loss:-0.490\n",
      "iteration:663 | d_loss:0.023 | g_loss:-0.490\n",
      "iteration:664 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:665 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:666 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:667 | d_loss:0.023 | g_loss:-0.490\n",
      "iteration:668 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:669 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:670 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:671 | d_loss:0.023 | g_loss:-0.490\n",
      "iteration:672 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:673 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:674 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:675 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:676 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:677 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:678 | d_loss:0.024 | g_loss:-0.490\n",
      "iteration:679 | d_loss:0.024 | g_loss:-0.491\n",
      "iteration:680 | d_loss:0.025 | g_loss:-0.490\n",
      "iteration:681 | d_loss:0.025 | g_loss:-0.490\n",
      "iteration:682 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:683 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:684 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:685 | d_loss:0.025 | g_loss:-0.491\n",
      "iteration:686 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:687 | d_loss:0.026 | g_loss:-0.491\n",
      "iteration:688 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:689 | d_loss:0.026 | g_loss:-0.492\n",
      "iteration:690 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:691 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:692 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:693 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:694 | d_loss:0.027 | g_loss:-0.492\n",
      "iteration:695 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:696 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:697 | d_loss:0.028 | g_loss:-0.492\n",
      "iteration:698 | d_loss:0.029 | g_loss:-0.492\n",
      "iteration:699 | d_loss:0.029 | g_loss:-0.492\n",
      "iteration:700 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:701 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:702 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:703 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:704 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:705 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:706 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:707 | d_loss:0.029 | g_loss:-0.493\n",
      "iteration:708 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:709 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:710 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:711 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:712 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:713 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:714 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:715 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:716 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:717 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:718 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:719 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:720 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:721 | d_loss:0.030 | g_loss:-0.493\n",
      "iteration:722 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:723 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:724 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:725 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:726 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:727 | d_loss:0.031 | g_loss:-0.493\n",
      "iteration:728 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:729 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:730 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:731 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:732 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:733 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:734 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:735 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:736 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:737 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:738 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:739 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:740 | d_loss:0.032 | g_loss:-0.493\n",
      "iteration:741 | d_loss:0.033 | g_loss:-0.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:742 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:743 | d_loss:0.034 | g_loss:-0.493\n",
      "iteration:744 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:745 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:746 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:747 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:748 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:749 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:750 | d_loss:0.033 | g_loss:-0.493\n",
      "iteration:751 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:752 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:753 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:754 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:755 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:756 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:757 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:758 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:759 | d_loss:0.033 | g_loss:-0.494\n",
      "iteration:760 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:761 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:762 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:763 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:764 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:765 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:766 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:767 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:768 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:769 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:770 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:771 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:772 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:773 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:774 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:775 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:776 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:777 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:778 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:779 | d_loss:0.034 | g_loss:-0.494\n",
      "iteration:780 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:781 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:782 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:783 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:784 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:785 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:786 | d_loss:0.034 | g_loss:-0.495\n",
      "iteration:787 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:788 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:789 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:790 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:791 | d_loss:0.035 | g_loss:-0.494\n",
      "iteration:792 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:793 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:794 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:795 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:796 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:797 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:798 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:799 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:800 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:801 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:802 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:803 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:804 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:805 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:806 | d_loss:0.035 | g_loss:-0.495\n",
      "iteration:807 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:808 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:809 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:810 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:811 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:812 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:813 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:814 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:815 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:816 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:817 | d_loss:0.036 | g_loss:-0.496\n",
      "iteration:818 | d_loss:0.036 | g_loss:-0.495\n",
      "iteration:819 | d_loss:0.036 | g_loss:-0.496\n",
      "iteration:820 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:821 | d_loss:0.036 | g_loss:-0.496\n",
      "iteration:822 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:823 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:824 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:825 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:826 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:827 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:828 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:829 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:830 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:831 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:832 | d_loss:0.037 | g_loss:-0.496\n",
      "iteration:833 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:834 | d_loss:0.038 | g_loss:-0.496\n",
      "iteration:835 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:836 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:837 | d_loss:0.037 | g_loss:-0.497\n",
      "iteration:838 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:839 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:840 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:841 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:842 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:843 | d_loss:0.039 | g_loss:-0.497\n",
      "iteration:844 | d_loss:0.039 | g_loss:-0.497\n",
      "iteration:845 | d_loss:0.038 | g_loss:-0.497\n",
      "iteration:846 | d_loss:0.039 | g_loss:-0.497\n",
      "iteration:847 | d_loss:0.039 | g_loss:-0.497\n",
      "iteration:848 | d_loss:0.039 | g_loss:-0.498\n",
      "iteration:849 | d_loss:0.039 | g_loss:-0.497\n",
      "iteration:850 | d_loss:0.039 | g_loss:-0.498\n",
      "iteration:851 | d_loss:0.039 | g_loss:-0.498\n",
      "iteration:852 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:853 | d_loss:0.039 | g_loss:-0.498\n",
      "iteration:854 | d_loss:0.040 | g_loss:-0.498\n",
      "iteration:855 | d_loss:0.040 | g_loss:-0.498\n",
      "iteration:856 | d_loss:0.040 | g_loss:-0.497\n",
      "iteration:857 | d_loss:0.040 | g_loss:-0.498\n",
      "iteration:858 | d_loss:0.040 | g_loss:-0.498\n",
      "iteration:859 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:860 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:861 | d_loss:0.040 | g_loss:-0.498\n",
      "iteration:862 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:863 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:864 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:865 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:866 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:867 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:868 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:869 | d_loss:0.041 | g_loss:-0.498\n",
      "iteration:870 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:871 | d_loss:0.041 | g_loss:-0.499\n",
      "iteration:872 | d_loss:0.042 | g_loss:-0.499\n",
      "iteration:873 | d_loss:0.042 | g_loss:-0.499\n",
      "iteration:874 | d_loss:0.042 | g_loss:-0.499\n",
      "iteration:875 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:876 | d_loss:0.042 | g_loss:-0.499\n",
      "iteration:877 | d_loss:0.042 | g_loss:-0.498\n",
      "iteration:878 | d_loss:0.043 | g_loss:-0.498\n",
      "iteration:879 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:880 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:881 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:882 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:883 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:884 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:885 | d_loss:0.044 | g_loss:-0.499\n",
      "iteration:886 | d_loss:0.043 | g_loss:-0.499\n",
      "iteration:887 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:888 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:889 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:890 | d_loss:0.044 | g_loss:-0.500\n",
      "iteration:891 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:892 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:893 | d_loss:0.045 | g_loss:-0.500\n",
      "iteration:894 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:895 | d_loss:0.045 | g_loss:-0.499\n",
      "iteration:896 | d_loss:0.046 | g_loss:-0.500\n",
      "iteration:897 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:898 | d_loss:0.046 | g_loss:-0.500\n",
      "iteration:899 | d_loss:0.046 | g_loss:-0.500\n",
      "iteration:900 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:901 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:902 | d_loss:0.047 | g_loss:-0.499\n",
      "iteration:903 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:904 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:905 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:906 | d_loss:0.047 | g_loss:-0.500\n",
      "iteration:907 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:908 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:909 | d_loss:0.049 | g_loss:-0.501\n",
      "iteration:910 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:911 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:912 | d_loss:0.048 | g_loss:-0.500\n",
      "iteration:913 | d_loss:0.049 | g_loss:-0.501\n",
      "iteration:914 | d_loss:0.050 | g_loss:-0.501\n",
      "iteration:915 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:916 | d_loss:0.049 | g_loss:-0.500\n",
      "iteration:917 | d_loss:0.050 | g_loss:-0.500\n",
      "iteration:918 | d_loss:0.051 | g_loss:-0.501\n",
      "iteration:919 | d_loss:0.050 | g_loss:-0.501\n",
      "iteration:920 | d_loss:0.051 | g_loss:-0.501\n",
      "iteration:921 | d_loss:0.051 | g_loss:-0.500\n",
      "iteration:922 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:923 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:924 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:925 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:926 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:927 | d_loss:0.052 | g_loss:-0.501\n",
      "iteration:928 | d_loss:0.053 | g_loss:-0.501\n",
      "iteration:929 | d_loss:0.052 | g_loss:-0.502\n",
      "iteration:930 | d_loss:0.053 | g_loss:-0.501\n",
      "iteration:931 | d_loss:0.053 | g_loss:-0.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:932 | d_loss:0.054 | g_loss:-0.502\n",
      "iteration:933 | d_loss:0.054 | g_loss:-0.501\n",
      "iteration:934 | d_loss:0.054 | g_loss:-0.502\n",
      "iteration:935 | d_loss:0.054 | g_loss:-0.502\n",
      "iteration:936 | d_loss:0.055 | g_loss:-0.503\n",
      "iteration:937 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:938 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:939 | d_loss:0.054 | g_loss:-0.502\n",
      "iteration:940 | d_loss:0.056 | g_loss:-0.502\n",
      "iteration:941 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:942 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:943 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:944 | d_loss:0.055 | g_loss:-0.502\n",
      "iteration:945 | d_loss:0.056 | g_loss:-0.502\n",
      "iteration:946 | d_loss:0.058 | g_loss:-0.502\n",
      "iteration:947 | d_loss:0.057 | g_loss:-0.502\n",
      "iteration:948 | d_loss:0.056 | g_loss:-0.502\n",
      "iteration:949 | d_loss:0.057 | g_loss:-0.503\n",
      "iteration:950 | d_loss:0.057 | g_loss:-0.503\n",
      "iteration:951 | d_loss:0.058 | g_loss:-0.503\n",
      "iteration:952 | d_loss:0.058 | g_loss:-0.503\n",
      "iteration:953 | d_loss:0.058 | g_loss:-0.503\n",
      "iteration:954 | d_loss:0.057 | g_loss:-0.503\n",
      "iteration:955 | d_loss:0.057 | g_loss:-0.503\n",
      "iteration:956 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:957 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:958 | d_loss:0.059 | g_loss:-0.502\n",
      "iteration:959 | d_loss:0.060 | g_loss:-0.503\n",
      "iteration:960 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:961 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:962 | d_loss:0.060 | g_loss:-0.503\n",
      "iteration:963 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:964 | d_loss:0.059 | g_loss:-0.503\n",
      "iteration:965 | d_loss:0.060 | g_loss:-0.503\n",
      "iteration:966 | d_loss:0.060 | g_loss:-0.502\n",
      "iteration:967 | d_loss:0.060 | g_loss:-0.503\n",
      "iteration:968 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:969 | d_loss:0.060 | g_loss:-0.503\n",
      "iteration:970 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:971 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:972 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:973 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:974 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:975 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:976 | d_loss:0.061 | g_loss:-0.503\n",
      "iteration:977 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:978 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:979 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:980 | d_loss:0.062 | g_loss:-0.502\n",
      "iteration:981 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:982 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:983 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:984 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:985 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:986 | d_loss:0.062 | g_loss:-0.503\n",
      "iteration:987 | d_loss:0.063 | g_loss:-0.503\n",
      "iteration:988 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:989 | d_loss:0.063 | g_loss:-0.503\n",
      "iteration:990 | d_loss:0.062 | g_loss:-0.502\n",
      "iteration:991 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:992 | d_loss:0.063 | g_loss:-0.503\n",
      "iteration:993 | d_loss:0.063 | g_loss:-0.503\n",
      "iteration:994 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:995 | d_loss:0.063 | g_loss:-0.503\n",
      "iteration:996 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:997 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:998 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:999 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1000 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:1001 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1002 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:1003 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1004 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1005 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:1006 | d_loss:0.063 | g_loss:-0.502\n",
      "iteration:1007 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1008 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1009 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1010 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1011 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1012 | d_loss:0.064 | g_loss:-0.502\n",
      "iteration:1013 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1014 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1015 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1016 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1017 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1018 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1019 | d_loss:0.065 | g_loss:-0.501\n",
      "iteration:1020 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1021 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1022 | d_loss:0.064 | g_loss:-0.501\n",
      "iteration:1023 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1024 | d_loss:0.063 | g_loss:-0.500\n",
      "iteration:1025 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1026 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1027 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1028 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1029 | d_loss:0.065 | g_loss:-0.500\n",
      "iteration:1030 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1031 | d_loss:0.065 | g_loss:-0.500\n",
      "iteration:1032 | d_loss:0.064 | g_loss:-0.500\n",
      "iteration:1033 | d_loss:0.065 | g_loss:-0.500\n",
      "iteration:1034 | d_loss:0.064 | g_loss:-0.499\n",
      "iteration:1035 | d_loss:0.064 | g_loss:-0.499\n",
      "iteration:1036 | d_loss:0.064 | g_loss:-0.499\n",
      "iteration:1037 | d_loss:0.063 | g_loss:-0.499\n",
      "iteration:1038 | d_loss:0.064 | g_loss:-0.499\n",
      "iteration:1039 | d_loss:0.064 | g_loss:-0.499\n",
      "iteration:1040 | d_loss:0.064 | g_loss:-0.498\n",
      "iteration:1041 | d_loss:0.065 | g_loss:-0.498\n",
      "iteration:1042 | d_loss:0.064 | g_loss:-0.498\n",
      "iteration:1043 | d_loss:0.064 | g_loss:-0.498\n",
      "iteration:1044 | d_loss:0.063 | g_loss:-0.498\n",
      "iteration:1045 | d_loss:0.065 | g_loss:-0.498\n",
      "iteration:1046 | d_loss:0.064 | g_loss:-0.498\n",
      "iteration:1047 | d_loss:0.064 | g_loss:-0.498\n",
      "iteration:1048 | d_loss:0.064 | g_loss:-0.497\n",
      "iteration:1049 | d_loss:0.064 | g_loss:-0.497\n",
      "iteration:1050 | d_loss:0.064 | g_loss:-0.497\n",
      "iteration:1051 | d_loss:0.065 | g_loss:-0.497\n",
      "iteration:1052 | d_loss:0.064 | g_loss:-0.497\n",
      "iteration:1053 | d_loss:0.065 | g_loss:-0.497\n",
      "iteration:1054 | d_loss:0.064 | g_loss:-0.497\n",
      "iteration:1055 | d_loss:0.065 | g_loss:-0.496\n",
      "iteration:1056 | d_loss:0.064 | g_loss:-0.496\n",
      "iteration:1057 | d_loss:0.064 | g_loss:-0.496\n",
      "iteration:1058 | d_loss:0.064 | g_loss:-0.496\n",
      "iteration:1059 | d_loss:0.065 | g_loss:-0.496\n",
      "iteration:1060 | d_loss:0.064 | g_loss:-0.496\n",
      "iteration:1061 | d_loss:0.064 | g_loss:-0.496\n",
      "iteration:1062 | d_loss:0.064 | g_loss:-0.495\n",
      "iteration:1063 | d_loss:0.063 | g_loss:-0.495\n",
      "iteration:1064 | d_loss:0.064 | g_loss:-0.495\n",
      "iteration:1065 | d_loss:0.063 | g_loss:-0.495\n",
      "iteration:1066 | d_loss:0.063 | g_loss:-0.495\n",
      "iteration:1067 | d_loss:0.063 | g_loss:-0.495\n",
      "iteration:1068 | d_loss:0.064 | g_loss:-0.494\n",
      "iteration:1069 | d_loss:0.063 | g_loss:-0.494\n",
      "iteration:1070 | d_loss:0.064 | g_loss:-0.494\n",
      "iteration:1071 | d_loss:0.063 | g_loss:-0.494\n",
      "iteration:1072 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1073 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1074 | d_loss:0.062 | g_loss:-0.493\n",
      "iteration:1075 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1076 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1077 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1078 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1079 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1080 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:1081 | d_loss:0.064 | g_loss:-0.492\n",
      "iteration:1082 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1083 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1084 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1085 | d_loss:0.063 | g_loss:-0.492\n",
      "iteration:1086 | d_loss:0.063 | g_loss:-0.491\n",
      "iteration:1087 | d_loss:0.063 | g_loss:-0.491\n",
      "iteration:1088 | d_loss:0.063 | g_loss:-0.491\n",
      "iteration:1089 | d_loss:0.063 | g_loss:-0.491\n",
      "iteration:1090 | d_loss:0.063 | g_loss:-0.491\n",
      "iteration:1091 | d_loss:0.062 | g_loss:-0.491\n",
      "iteration:1092 | d_loss:0.063 | g_loss:-0.490\n",
      "iteration:1093 | d_loss:0.063 | g_loss:-0.490\n",
      "iteration:1094 | d_loss:0.062 | g_loss:-0.490\n",
      "iteration:1095 | d_loss:0.062 | g_loss:-0.490\n",
      "iteration:1096 | d_loss:0.062 | g_loss:-0.490\n",
      "iteration:1097 | d_loss:0.062 | g_loss:-0.490\n",
      "iteration:1098 | d_loss:0.062 | g_loss:-0.489\n",
      "iteration:1099 | d_loss:0.062 | g_loss:-0.489\n",
      "iteration:1100 | d_loss:0.063 | g_loss:-0.489\n",
      "iteration:1101 | d_loss:0.063 | g_loss:-0.489\n",
      "iteration:1102 | d_loss:0.063 | g_loss:-0.489\n",
      "iteration:1103 | d_loss:0.062 | g_loss:-0.489\n",
      "iteration:1104 | d_loss:0.063 | g_loss:-0.489\n",
      "iteration:1105 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1106 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1107 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1108 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1109 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1110 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:1111 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:1112 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:1113 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:1114 | d_loss:0.062 | g_loss:-0.487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1115 | d_loss:0.061 | g_loss:-0.487\n",
      "iteration:1116 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:1117 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:1118 | d_loss:0.063 | g_loss:-0.486\n",
      "iteration:1119 | d_loss:0.062 | g_loss:-0.486\n",
      "iteration:1120 | d_loss:0.062 | g_loss:-0.486\n",
      "iteration:1121 | d_loss:0.061 | g_loss:-0.486\n",
      "iteration:1122 | d_loss:0.062 | g_loss:-0.486\n",
      "iteration:1123 | d_loss:0.062 | g_loss:-0.485\n",
      "iteration:1124 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:1125 | d_loss:0.062 | g_loss:-0.485\n",
      "iteration:1126 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:1127 | d_loss:0.062 | g_loss:-0.485\n",
      "iteration:1128 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:1129 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:1130 | d_loss:0.061 | g_loss:-0.484\n",
      "iteration:1131 | d_loss:0.061 | g_loss:-0.484\n",
      "iteration:1132 | d_loss:0.061 | g_loss:-0.484\n",
      "iteration:1133 | d_loss:0.061 | g_loss:-0.484\n",
      "iteration:1134 | d_loss:0.061 | g_loss:-0.483\n",
      "iteration:1135 | d_loss:0.061 | g_loss:-0.483\n",
      "iteration:1136 | d_loss:0.061 | g_loss:-0.483\n",
      "iteration:1137 | d_loss:0.061 | g_loss:-0.483\n",
      "iteration:1138 | d_loss:0.061 | g_loss:-0.482\n",
      "iteration:1139 | d_loss:0.061 | g_loss:-0.482\n",
      "iteration:1140 | d_loss:0.061 | g_loss:-0.482\n",
      "iteration:1141 | d_loss:0.060 | g_loss:-0.482\n",
      "iteration:1142 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:1143 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:1144 | d_loss:0.061 | g_loss:-0.481\n",
      "iteration:1145 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:1146 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:1147 | d_loss:0.061 | g_loss:-0.481\n",
      "iteration:1148 | d_loss:0.060 | g_loss:-0.480\n",
      "iteration:1149 | d_loss:0.060 | g_loss:-0.480\n",
      "iteration:1150 | d_loss:0.060 | g_loss:-0.480\n",
      "iteration:1151 | d_loss:0.060 | g_loss:-0.479\n",
      "iteration:1152 | d_loss:0.060 | g_loss:-0.479\n",
      "iteration:1153 | d_loss:0.059 | g_loss:-0.479\n",
      "iteration:1154 | d_loss:0.059 | g_loss:-0.479\n",
      "iteration:1155 | d_loss:0.060 | g_loss:-0.479\n",
      "iteration:1156 | d_loss:0.059 | g_loss:-0.478\n",
      "iteration:1157 | d_loss:0.059 | g_loss:-0.478\n",
      "iteration:1158 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:1159 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:1160 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:1161 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:1162 | d_loss:0.058 | g_loss:-0.476\n",
      "iteration:1163 | d_loss:0.059 | g_loss:-0.476\n",
      "iteration:1164 | d_loss:0.058 | g_loss:-0.476\n",
      "iteration:1165 | d_loss:0.058 | g_loss:-0.476\n",
      "iteration:1166 | d_loss:0.058 | g_loss:-0.475\n",
      "iteration:1167 | d_loss:0.058 | g_loss:-0.475\n",
      "iteration:1168 | d_loss:0.058 | g_loss:-0.475\n",
      "iteration:1169 | d_loss:0.058 | g_loss:-0.474\n",
      "iteration:1170 | d_loss:0.058 | g_loss:-0.474\n",
      "iteration:1171 | d_loss:0.058 | g_loss:-0.474\n",
      "iteration:1172 | d_loss:0.058 | g_loss:-0.474\n",
      "iteration:1173 | d_loss:0.058 | g_loss:-0.473\n",
      "iteration:1174 | d_loss:0.057 | g_loss:-0.473\n",
      "iteration:1175 | d_loss:0.057 | g_loss:-0.473\n",
      "iteration:1176 | d_loss:0.057 | g_loss:-0.473\n",
      "iteration:1177 | d_loss:0.057 | g_loss:-0.472\n",
      "iteration:1178 | d_loss:0.057 | g_loss:-0.472\n",
      "iteration:1179 | d_loss:0.056 | g_loss:-0.471\n",
      "iteration:1180 | d_loss:0.057 | g_loss:-0.471\n",
      "iteration:1181 | d_loss:0.057 | g_loss:-0.471\n",
      "iteration:1182 | d_loss:0.057 | g_loss:-0.471\n",
      "iteration:1183 | d_loss:0.056 | g_loss:-0.470\n",
      "iteration:1184 | d_loss:0.056 | g_loss:-0.470\n",
      "iteration:1185 | d_loss:0.056 | g_loss:-0.470\n",
      "iteration:1186 | d_loss:0.056 | g_loss:-0.469\n",
      "iteration:1187 | d_loss:0.056 | g_loss:-0.469\n",
      "iteration:1188 | d_loss:0.056 | g_loss:-0.469\n",
      "iteration:1189 | d_loss:0.056 | g_loss:-0.469\n",
      "iteration:1190 | d_loss:0.055 | g_loss:-0.468\n",
      "iteration:1191 | d_loss:0.055 | g_loss:-0.468\n",
      "iteration:1192 | d_loss:0.055 | g_loss:-0.468\n",
      "iteration:1193 | d_loss:0.055 | g_loss:-0.467\n",
      "iteration:1194 | d_loss:0.055 | g_loss:-0.467\n",
      "iteration:1195 | d_loss:0.055 | g_loss:-0.467\n",
      "iteration:1196 | d_loss:0.055 | g_loss:-0.466\n",
      "iteration:1197 | d_loss:0.054 | g_loss:-0.466\n",
      "iteration:1198 | d_loss:0.055 | g_loss:-0.466\n",
      "iteration:1199 | d_loss:0.055 | g_loss:-0.466\n",
      "iteration:1200 | d_loss:0.054 | g_loss:-0.465\n",
      "iteration:1201 | d_loss:0.054 | g_loss:-0.465\n",
      "iteration:1202 | d_loss:0.054 | g_loss:-0.465\n",
      "iteration:1203 | d_loss:0.054 | g_loss:-0.465\n",
      "iteration:1204 | d_loss:0.054 | g_loss:-0.464\n",
      "iteration:1205 | d_loss:0.053 | g_loss:-0.464\n",
      "iteration:1206 | d_loss:0.054 | g_loss:-0.463\n",
      "iteration:1207 | d_loss:0.053 | g_loss:-0.463\n",
      "iteration:1208 | d_loss:0.054 | g_loss:-0.463\n",
      "iteration:1209 | d_loss:0.053 | g_loss:-0.463\n",
      "iteration:1210 | d_loss:0.053 | g_loss:-0.462\n",
      "iteration:1211 | d_loss:0.053 | g_loss:-0.462\n",
      "iteration:1212 | d_loss:0.053 | g_loss:-0.462\n",
      "iteration:1213 | d_loss:0.053 | g_loss:-0.461\n",
      "iteration:1214 | d_loss:0.053 | g_loss:-0.461\n",
      "iteration:1215 | d_loss:0.052 | g_loss:-0.461\n",
      "iteration:1216 | d_loss:0.053 | g_loss:-0.461\n",
      "iteration:1217 | d_loss:0.052 | g_loss:-0.460\n",
      "iteration:1218 | d_loss:0.052 | g_loss:-0.460\n",
      "iteration:1219 | d_loss:0.052 | g_loss:-0.460\n",
      "iteration:1220 | d_loss:0.052 | g_loss:-0.459\n",
      "iteration:1221 | d_loss:0.052 | g_loss:-0.459\n",
      "iteration:1222 | d_loss:0.052 | g_loss:-0.459\n",
      "iteration:1223 | d_loss:0.052 | g_loss:-0.458\n",
      "iteration:1224 | d_loss:0.051 | g_loss:-0.458\n",
      "iteration:1225 | d_loss:0.052 | g_loss:-0.458\n",
      "iteration:1226 | d_loss:0.051 | g_loss:-0.458\n",
      "iteration:1227 | d_loss:0.051 | g_loss:-0.458\n",
      "iteration:1228 | d_loss:0.051 | g_loss:-0.457\n",
      "iteration:1229 | d_loss:0.051 | g_loss:-0.457\n",
      "iteration:1230 | d_loss:0.050 | g_loss:-0.457\n",
      "iteration:1231 | d_loss:0.051 | g_loss:-0.456\n",
      "iteration:1232 | d_loss:0.050 | g_loss:-0.456\n",
      "iteration:1233 | d_loss:0.050 | g_loss:-0.456\n",
      "iteration:1234 | d_loss:0.050 | g_loss:-0.455\n",
      "iteration:1235 | d_loss:0.050 | g_loss:-0.455\n",
      "iteration:1236 | d_loss:0.050 | g_loss:-0.455\n",
      "iteration:1237 | d_loss:0.049 | g_loss:-0.454\n",
      "iteration:1238 | d_loss:0.049 | g_loss:-0.454\n",
      "iteration:1239 | d_loss:0.049 | g_loss:-0.454\n",
      "iteration:1240 | d_loss:0.049 | g_loss:-0.454\n",
      "iteration:1241 | d_loss:0.049 | g_loss:-0.454\n",
      "iteration:1242 | d_loss:0.049 | g_loss:-0.453\n",
      "iteration:1243 | d_loss:0.048 | g_loss:-0.453\n",
      "iteration:1244 | d_loss:0.049 | g_loss:-0.453\n",
      "iteration:1245 | d_loss:0.048 | g_loss:-0.453\n",
      "iteration:1246 | d_loss:0.048 | g_loss:-0.453\n",
      "iteration:1247 | d_loss:0.048 | g_loss:-0.453\n",
      "iteration:1248 | d_loss:0.048 | g_loss:-0.452\n",
      "iteration:1249 | d_loss:0.048 | g_loss:-0.452\n",
      "iteration:1250 | d_loss:0.047 | g_loss:-0.452\n",
      "iteration:1251 | d_loss:0.047 | g_loss:-0.452\n",
      "iteration:1252 | d_loss:0.047 | g_loss:-0.452\n",
      "iteration:1253 | d_loss:0.047 | g_loss:-0.451\n",
      "iteration:1254 | d_loss:0.047 | g_loss:-0.452\n",
      "iteration:1255 | d_loss:0.047 | g_loss:-0.451\n",
      "iteration:1256 | d_loss:0.047 | g_loss:-0.451\n",
      "iteration:1257 | d_loss:0.046 | g_loss:-0.451\n",
      "iteration:1258 | d_loss:0.046 | g_loss:-0.451\n",
      "iteration:1259 | d_loss:0.046 | g_loss:-0.450\n",
      "iteration:1260 | d_loss:0.046 | g_loss:-0.450\n",
      "iteration:1261 | d_loss:0.045 | g_loss:-0.450\n",
      "iteration:1262 | d_loss:0.045 | g_loss:-0.450\n",
      "iteration:1263 | d_loss:0.045 | g_loss:-0.450\n",
      "iteration:1264 | d_loss:0.045 | g_loss:-0.450\n",
      "iteration:1265 | d_loss:0.045 | g_loss:-0.450\n",
      "iteration:1266 | d_loss:0.045 | g_loss:-0.449\n",
      "iteration:1267 | d_loss:0.044 | g_loss:-0.449\n",
      "iteration:1268 | d_loss:0.044 | g_loss:-0.449\n",
      "iteration:1269 | d_loss:0.044 | g_loss:-0.449\n",
      "iteration:1270 | d_loss:0.044 | g_loss:-0.449\n",
      "iteration:1271 | d_loss:0.044 | g_loss:-0.448\n",
      "iteration:1272 | d_loss:0.044 | g_loss:-0.448\n",
      "iteration:1273 | d_loss:0.044 | g_loss:-0.448\n",
      "iteration:1274 | d_loss:0.043 | g_loss:-0.448\n",
      "iteration:1275 | d_loss:0.043 | g_loss:-0.448\n",
      "iteration:1276 | d_loss:0.043 | g_loss:-0.447\n",
      "iteration:1277 | d_loss:0.043 | g_loss:-0.447\n",
      "iteration:1278 | d_loss:0.042 | g_loss:-0.447\n",
      "iteration:1279 | d_loss:0.042 | g_loss:-0.447\n",
      "iteration:1280 | d_loss:0.042 | g_loss:-0.446\n",
      "iteration:1281 | d_loss:0.042 | g_loss:-0.446\n",
      "iteration:1282 | d_loss:0.041 | g_loss:-0.446\n",
      "iteration:1283 | d_loss:0.041 | g_loss:-0.446\n",
      "iteration:1284 | d_loss:0.041 | g_loss:-0.446\n",
      "iteration:1285 | d_loss:0.041 | g_loss:-0.446\n",
      "iteration:1286 | d_loss:0.041 | g_loss:-0.445\n",
      "iteration:1287 | d_loss:0.041 | g_loss:-0.445\n",
      "iteration:1288 | d_loss:0.041 | g_loss:-0.445\n",
      "iteration:1289 | d_loss:0.040 | g_loss:-0.445\n",
      "iteration:1290 | d_loss:0.040 | g_loss:-0.444\n",
      "iteration:1291 | d_loss:0.040 | g_loss:-0.444\n",
      "iteration:1292 | d_loss:0.040 | g_loss:-0.444\n",
      "iteration:1293 | d_loss:0.040 | g_loss:-0.444\n",
      "iteration:1294 | d_loss:0.039 | g_loss:-0.444\n",
      "iteration:1295 | d_loss:0.039 | g_loss:-0.443\n",
      "iteration:1296 | d_loss:0.039 | g_loss:-0.443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1297 | d_loss:0.039 | g_loss:-0.443\n",
      "iteration:1298 | d_loss:0.039 | g_loss:-0.443\n",
      "iteration:1299 | d_loss:0.038 | g_loss:-0.443\n",
      "iteration:1300 | d_loss:0.038 | g_loss:-0.442\n",
      "iteration:1301 | d_loss:0.038 | g_loss:-0.442\n",
      "iteration:1302 | d_loss:0.037 | g_loss:-0.442\n",
      "iteration:1303 | d_loss:0.038 | g_loss:-0.442\n",
      "iteration:1304 | d_loss:0.037 | g_loss:-0.442\n",
      "iteration:1305 | d_loss:0.037 | g_loss:-0.441\n",
      "iteration:1306 | d_loss:0.037 | g_loss:-0.441\n",
      "iteration:1307 | d_loss:0.036 | g_loss:-0.441\n",
      "iteration:1308 | d_loss:0.036 | g_loss:-0.441\n",
      "iteration:1309 | d_loss:0.036 | g_loss:-0.440\n",
      "iteration:1310 | d_loss:0.036 | g_loss:-0.440\n",
      "iteration:1311 | d_loss:0.036 | g_loss:-0.440\n",
      "iteration:1312 | d_loss:0.036 | g_loss:-0.440\n",
      "iteration:1313 | d_loss:0.036 | g_loss:-0.439\n",
      "iteration:1314 | d_loss:0.035 | g_loss:-0.439\n",
      "iteration:1315 | d_loss:0.035 | g_loss:-0.439\n",
      "iteration:1316 | d_loss:0.034 | g_loss:-0.439\n",
      "iteration:1317 | d_loss:0.034 | g_loss:-0.439\n",
      "iteration:1318 | d_loss:0.034 | g_loss:-0.438\n",
      "iteration:1319 | d_loss:0.034 | g_loss:-0.438\n",
      "iteration:1320 | d_loss:0.034 | g_loss:-0.438\n",
      "iteration:1321 | d_loss:0.034 | g_loss:-0.438\n",
      "iteration:1322 | d_loss:0.033 | g_loss:-0.438\n",
      "iteration:1323 | d_loss:0.033 | g_loss:-0.437\n",
      "iteration:1324 | d_loss:0.033 | g_loss:-0.437\n",
      "iteration:1325 | d_loss:0.033 | g_loss:-0.437\n",
      "iteration:1326 | d_loss:0.032 | g_loss:-0.437\n",
      "iteration:1327 | d_loss:0.032 | g_loss:-0.436\n",
      "iteration:1328 | d_loss:0.032 | g_loss:-0.436\n",
      "iteration:1329 | d_loss:0.032 | g_loss:-0.436\n",
      "iteration:1330 | d_loss:0.031 | g_loss:-0.436\n",
      "iteration:1331 | d_loss:0.031 | g_loss:-0.435\n",
      "iteration:1332 | d_loss:0.031 | g_loss:-0.435\n",
      "iteration:1333 | d_loss:0.031 | g_loss:-0.435\n",
      "iteration:1334 | d_loss:0.031 | g_loss:-0.435\n",
      "iteration:1335 | d_loss:0.030 | g_loss:-0.434\n",
      "iteration:1336 | d_loss:0.030 | g_loss:-0.434\n",
      "iteration:1337 | d_loss:0.030 | g_loss:-0.434\n",
      "iteration:1338 | d_loss:0.029 | g_loss:-0.434\n",
      "iteration:1339 | d_loss:0.029 | g_loss:-0.433\n",
      "iteration:1340 | d_loss:0.029 | g_loss:-0.433\n",
      "iteration:1341 | d_loss:0.029 | g_loss:-0.433\n",
      "iteration:1342 | d_loss:0.029 | g_loss:-0.432\n",
      "iteration:1343 | d_loss:0.028 | g_loss:-0.432\n",
      "iteration:1344 | d_loss:0.028 | g_loss:-0.432\n",
      "iteration:1345 | d_loss:0.028 | g_loss:-0.432\n",
      "iteration:1346 | d_loss:0.027 | g_loss:-0.431\n",
      "iteration:1347 | d_loss:0.027 | g_loss:-0.431\n",
      "iteration:1348 | d_loss:0.027 | g_loss:-0.431\n",
      "iteration:1349 | d_loss:0.027 | g_loss:-0.431\n",
      "iteration:1350 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:1351 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:1352 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:1353 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:1354 | d_loss:0.026 | g_loss:-0.429\n",
      "iteration:1355 | d_loss:0.025 | g_loss:-0.429\n",
      "iteration:1356 | d_loss:0.025 | g_loss:-0.429\n",
      "iteration:1357 | d_loss:0.025 | g_loss:-0.429\n",
      "iteration:1358 | d_loss:0.024 | g_loss:-0.428\n",
      "iteration:1359 | d_loss:0.024 | g_loss:-0.428\n",
      "iteration:1360 | d_loss:0.024 | g_loss:-0.428\n",
      "iteration:1361 | d_loss:0.023 | g_loss:-0.427\n",
      "iteration:1362 | d_loss:0.023 | g_loss:-0.427\n",
      "iteration:1363 | d_loss:0.023 | g_loss:-0.427\n",
      "iteration:1364 | d_loss:0.022 | g_loss:-0.427\n",
      "iteration:1365 | d_loss:0.022 | g_loss:-0.426\n",
      "iteration:1366 | d_loss:0.022 | g_loss:-0.426\n",
      "iteration:1367 | d_loss:0.022 | g_loss:-0.426\n",
      "iteration:1368 | d_loss:0.021 | g_loss:-0.425\n",
      "iteration:1369 | d_loss:0.021 | g_loss:-0.425\n",
      "iteration:1370 | d_loss:0.021 | g_loss:-0.425\n",
      "iteration:1371 | d_loss:0.021 | g_loss:-0.425\n",
      "iteration:1372 | d_loss:0.020 | g_loss:-0.424\n",
      "iteration:1373 | d_loss:0.020 | g_loss:-0.424\n",
      "iteration:1374 | d_loss:0.020 | g_loss:-0.424\n",
      "iteration:1375 | d_loss:0.020 | g_loss:-0.423\n",
      "iteration:1376 | d_loss:0.019 | g_loss:-0.423\n",
      "iteration:1377 | d_loss:0.019 | g_loss:-0.423\n",
      "iteration:1378 | d_loss:0.018 | g_loss:-0.422\n",
      "iteration:1379 | d_loss:0.018 | g_loss:-0.422\n",
      "iteration:1380 | d_loss:0.018 | g_loss:-0.422\n",
      "iteration:1381 | d_loss:0.018 | g_loss:-0.422\n",
      "iteration:1382 | d_loss:0.017 | g_loss:-0.421\n",
      "iteration:1383 | d_loss:0.017 | g_loss:-0.421\n",
      "iteration:1384 | d_loss:0.017 | g_loss:-0.421\n",
      "iteration:1385 | d_loss:0.017 | g_loss:-0.420\n",
      "iteration:1386 | d_loss:0.016 | g_loss:-0.420\n",
      "iteration:1387 | d_loss:0.016 | g_loss:-0.420\n",
      "iteration:1388 | d_loss:0.015 | g_loss:-0.420\n",
      "iteration:1389 | d_loss:0.015 | g_loss:-0.419\n",
      "iteration:1390 | d_loss:0.015 | g_loss:-0.419\n",
      "iteration:1391 | d_loss:0.015 | g_loss:-0.419\n",
      "iteration:1392 | d_loss:0.015 | g_loss:-0.419\n",
      "iteration:1393 | d_loss:0.014 | g_loss:-0.418\n",
      "iteration:1394 | d_loss:0.014 | g_loss:-0.418\n",
      "iteration:1395 | d_loss:0.014 | g_loss:-0.418\n",
      "iteration:1396 | d_loss:0.013 | g_loss:-0.417\n",
      "iteration:1397 | d_loss:0.013 | g_loss:-0.417\n",
      "iteration:1398 | d_loss:0.013 | g_loss:-0.417\n",
      "iteration:1399 | d_loss:0.012 | g_loss:-0.417\n",
      "iteration:1400 | d_loss:0.012 | g_loss:-0.416\n",
      "iteration:1401 | d_loss:0.012 | g_loss:-0.416\n",
      "iteration:1402 | d_loss:0.011 | g_loss:-0.416\n",
      "iteration:1403 | d_loss:0.011 | g_loss:-0.415\n",
      "iteration:1404 | d_loss:0.011 | g_loss:-0.415\n",
      "iteration:1405 | d_loss:0.011 | g_loss:-0.415\n",
      "iteration:1406 | d_loss:0.010 | g_loss:-0.415\n",
      "iteration:1407 | d_loss:0.010 | g_loss:-0.414\n",
      "iteration:1408 | d_loss:0.010 | g_loss:-0.414\n",
      "iteration:1409 | d_loss:0.009 | g_loss:-0.414\n",
      "iteration:1410 | d_loss:0.009 | g_loss:-0.414\n",
      "iteration:1411 | d_loss:0.009 | g_loss:-0.413\n",
      "iteration:1412 | d_loss:0.009 | g_loss:-0.413\n",
      "iteration:1413 | d_loss:0.008 | g_loss:-0.413\n",
      "iteration:1414 | d_loss:0.008 | g_loss:-0.413\n",
      "iteration:1415 | d_loss:0.008 | g_loss:-0.412\n",
      "iteration:1416 | d_loss:0.007 | g_loss:-0.412\n",
      "iteration:1417 | d_loss:0.007 | g_loss:-0.412\n",
      "iteration:1418 | d_loss:0.007 | g_loss:-0.412\n",
      "iteration:1419 | d_loss:0.007 | g_loss:-0.411\n",
      "iteration:1420 | d_loss:0.006 | g_loss:-0.411\n",
      "iteration:1421 | d_loss:0.006 | g_loss:-0.411\n",
      "iteration:1422 | d_loss:0.005 | g_loss:-0.411\n",
      "iteration:1423 | d_loss:0.005 | g_loss:-0.410\n",
      "iteration:1424 | d_loss:0.005 | g_loss:-0.410\n",
      "iteration:1425 | d_loss:0.005 | g_loss:-0.410\n",
      "iteration:1426 | d_loss:0.004 | g_loss:-0.410\n",
      "iteration:1427 | d_loss:0.004 | g_loss:-0.409\n",
      "iteration:1428 | d_loss:0.004 | g_loss:-0.409\n",
      "iteration:1429 | d_loss:0.004 | g_loss:-0.409\n",
      "iteration:1430 | d_loss:0.003 | g_loss:-0.409\n",
      "iteration:1431 | d_loss:0.003 | g_loss:-0.408\n",
      "iteration:1432 | d_loss:0.002 | g_loss:-0.408\n",
      "iteration:1433 | d_loss:0.002 | g_loss:-0.408\n",
      "iteration:1434 | d_loss:0.002 | g_loss:-0.408\n",
      "iteration:1435 | d_loss:0.002 | g_loss:-0.408\n",
      "iteration:1436 | d_loss:0.001 | g_loss:-0.407\n",
      "iteration:1437 | d_loss:0.002 | g_loss:-0.407\n",
      "iteration:1438 | d_loss:0.001 | g_loss:-0.407\n",
      "iteration:1439 | d_loss:0.001 | g_loss:-0.407\n",
      "iteration:1440 | d_loss:0.001 | g_loss:-0.407\n",
      "iteration:1441 | d_loss:0.000 | g_loss:-0.406\n",
      "iteration:1442 | d_loss:-0.000 | g_loss:-0.406\n",
      "iteration:1443 | d_loss:-0.000 | g_loss:-0.406\n",
      "iteration:1444 | d_loss:-0.001 | g_loss:-0.406\n",
      "iteration:1445 | d_loss:-0.001 | g_loss:-0.406\n",
      "iteration:1446 | d_loss:-0.001 | g_loss:-0.406\n",
      "iteration:1447 | d_loss:-0.001 | g_loss:-0.405\n",
      "iteration:1448 | d_loss:-0.001 | g_loss:-0.405\n",
      "iteration:1449 | d_loss:-0.002 | g_loss:-0.405\n",
      "iteration:1450 | d_loss:-0.002 | g_loss:-0.405\n",
      "iteration:1451 | d_loss:-0.002 | g_loss:-0.404\n",
      "iteration:1452 | d_loss:-0.003 | g_loss:-0.404\n",
      "iteration:1453 | d_loss:-0.003 | g_loss:-0.404\n",
      "iteration:1454 | d_loss:-0.003 | g_loss:-0.404\n",
      "iteration:1455 | d_loss:-0.003 | g_loss:-0.404\n",
      "iteration:1456 | d_loss:-0.004 | g_loss:-0.404\n",
      "iteration:1457 | d_loss:-0.004 | g_loss:-0.404\n",
      "iteration:1458 | d_loss:-0.004 | g_loss:-0.403\n",
      "iteration:1459 | d_loss:-0.004 | g_loss:-0.403\n",
      "iteration:1460 | d_loss:-0.005 | g_loss:-0.403\n",
      "iteration:1461 | d_loss:-0.005 | g_loss:-0.403\n",
      "iteration:1462 | d_loss:-0.005 | g_loss:-0.403\n",
      "iteration:1463 | d_loss:-0.006 | g_loss:-0.402\n",
      "iteration:1464 | d_loss:-0.006 | g_loss:-0.402\n",
      "iteration:1465 | d_loss:-0.006 | g_loss:-0.402\n",
      "iteration:1466 | d_loss:-0.006 | g_loss:-0.402\n",
      "iteration:1467 | d_loss:-0.006 | g_loss:-0.402\n",
      "iteration:1468 | d_loss:-0.007 | g_loss:-0.402\n",
      "iteration:1469 | d_loss:-0.007 | g_loss:-0.401\n",
      "iteration:1470 | d_loss:-0.007 | g_loss:-0.401\n",
      "iteration:1471 | d_loss:-0.007 | g_loss:-0.401\n",
      "iteration:1472 | d_loss:-0.008 | g_loss:-0.401\n",
      "iteration:1473 | d_loss:-0.008 | g_loss:-0.401\n",
      "iteration:1474 | d_loss:-0.008 | g_loss:-0.400\n",
      "iteration:1475 | d_loss:-0.009 | g_loss:-0.400\n",
      "iteration:1476 | d_loss:-0.009 | g_loss:-0.400\n",
      "iteration:1477 | d_loss:-0.009 | g_loss:-0.400\n",
      "iteration:1478 | d_loss:-0.009 | g_loss:-0.400\n",
      "iteration:1479 | d_loss:-0.010 | g_loss:-0.400\n",
      "iteration:1480 | d_loss:-0.010 | g_loss:-0.400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1481 | d_loss:-0.010 | g_loss:-0.399\n",
      "iteration:1482 | d_loss:-0.011 | g_loss:-0.399\n",
      "iteration:1483 | d_loss:-0.011 | g_loss:-0.399\n",
      "iteration:1484 | d_loss:-0.011 | g_loss:-0.399\n",
      "iteration:1485 | d_loss:-0.011 | g_loss:-0.399\n",
      "iteration:1486 | d_loss:-0.011 | g_loss:-0.399\n",
      "iteration:1487 | d_loss:-0.012 | g_loss:-0.398\n",
      "iteration:1488 | d_loss:-0.012 | g_loss:-0.398\n",
      "iteration:1489 | d_loss:-0.012 | g_loss:-0.398\n",
      "iteration:1490 | d_loss:-0.013 | g_loss:-0.398\n",
      "iteration:1491 | d_loss:-0.013 | g_loss:-0.398\n",
      "iteration:1492 | d_loss:-0.013 | g_loss:-0.398\n",
      "iteration:1493 | d_loss:-0.014 | g_loss:-0.398\n",
      "iteration:1494 | d_loss:-0.014 | g_loss:-0.398\n",
      "iteration:1495 | d_loss:-0.014 | g_loss:-0.397\n",
      "iteration:1496 | d_loss:-0.014 | g_loss:-0.397\n",
      "iteration:1497 | d_loss:-0.014 | g_loss:-0.397\n",
      "iteration:1498 | d_loss:-0.015 | g_loss:-0.397\n",
      "iteration:1499 | d_loss:-0.015 | g_loss:-0.397\n",
      "iteration:1500 | d_loss:-0.015 | g_loss:-0.397\n",
      "iteration:1501 | d_loss:-0.015 | g_loss:-0.396\n",
      "iteration:1502 | d_loss:-0.015 | g_loss:-0.396\n",
      "iteration:1503 | d_loss:-0.016 | g_loss:-0.396\n",
      "iteration:1504 | d_loss:-0.016 | g_loss:-0.396\n",
      "iteration:1505 | d_loss:-0.016 | g_loss:-0.396\n",
      "iteration:1506 | d_loss:-0.016 | g_loss:-0.396\n",
      "iteration:1507 | d_loss:-0.016 | g_loss:-0.395\n",
      "iteration:1508 | d_loss:-0.017 | g_loss:-0.395\n",
      "iteration:1509 | d_loss:-0.017 | g_loss:-0.395\n",
      "iteration:1510 | d_loss:-0.017 | g_loss:-0.395\n",
      "iteration:1511 | d_loss:-0.017 | g_loss:-0.395\n",
      "iteration:1512 | d_loss:-0.018 | g_loss:-0.394\n",
      "iteration:1513 | d_loss:-0.018 | g_loss:-0.394\n",
      "iteration:1514 | d_loss:-0.018 | g_loss:-0.394\n",
      "iteration:1515 | d_loss:-0.018 | g_loss:-0.394\n",
      "iteration:1516 | d_loss:-0.019 | g_loss:-0.394\n",
      "iteration:1517 | d_loss:-0.019 | g_loss:-0.394\n",
      "iteration:1518 | d_loss:-0.019 | g_loss:-0.394\n",
      "iteration:1519 | d_loss:-0.020 | g_loss:-0.393\n",
      "iteration:1520 | d_loss:-0.019 | g_loss:-0.393\n",
      "iteration:1521 | d_loss:-0.020 | g_loss:-0.393\n",
      "iteration:1522 | d_loss:-0.020 | g_loss:-0.393\n",
      "iteration:1523 | d_loss:-0.020 | g_loss:-0.392\n",
      "iteration:1524 | d_loss:-0.020 | g_loss:-0.392\n",
      "iteration:1525 | d_loss:-0.020 | g_loss:-0.392\n",
      "iteration:1526 | d_loss:-0.021 | g_loss:-0.392\n",
      "iteration:1527 | d_loss:-0.021 | g_loss:-0.392\n",
      "iteration:1528 | d_loss:-0.021 | g_loss:-0.392\n",
      "iteration:1529 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1530 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1531 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1532 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1533 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1534 | d_loss:-0.022 | g_loss:-0.391\n",
      "iteration:1535 | d_loss:-0.022 | g_loss:-0.390\n",
      "iteration:1536 | d_loss:-0.023 | g_loss:-0.390\n",
      "iteration:1537 | d_loss:-0.023 | g_loss:-0.390\n",
      "iteration:1538 | d_loss:-0.023 | g_loss:-0.390\n",
      "iteration:1539 | d_loss:-0.023 | g_loss:-0.390\n",
      "iteration:1540 | d_loss:-0.023 | g_loss:-0.390\n",
      "iteration:1541 | d_loss:-0.024 | g_loss:-0.390\n",
      "iteration:1542 | d_loss:-0.024 | g_loss:-0.389\n",
      "iteration:1543 | d_loss:-0.024 | g_loss:-0.389\n",
      "iteration:1544 | d_loss:-0.025 | g_loss:-0.389\n",
      "iteration:1545 | d_loss:-0.025 | g_loss:-0.389\n",
      "iteration:1546 | d_loss:-0.025 | g_loss:-0.389\n",
      "iteration:1547 | d_loss:-0.025 | g_loss:-0.389\n",
      "iteration:1548 | d_loss:-0.025 | g_loss:-0.389\n",
      "iteration:1549 | d_loss:-0.026 | g_loss:-0.388\n",
      "iteration:1550 | d_loss:-0.026 | g_loss:-0.388\n",
      "iteration:1551 | d_loss:-0.026 | g_loss:-0.388\n",
      "iteration:1552 | d_loss:-0.026 | g_loss:-0.388\n",
      "iteration:1553 | d_loss:-0.026 | g_loss:-0.388\n",
      "iteration:1554 | d_loss:-0.027 | g_loss:-0.387\n",
      "iteration:1555 | d_loss:-0.026 | g_loss:-0.387\n",
      "iteration:1556 | d_loss:-0.027 | g_loss:-0.387\n",
      "iteration:1557 | d_loss:-0.027 | g_loss:-0.387\n",
      "iteration:1558 | d_loss:-0.027 | g_loss:-0.387\n",
      "iteration:1559 | d_loss:-0.027 | g_loss:-0.387\n",
      "iteration:1560 | d_loss:-0.028 | g_loss:-0.387\n",
      "iteration:1561 | d_loss:-0.028 | g_loss:-0.386\n",
      "iteration:1562 | d_loss:-0.028 | g_loss:-0.386\n",
      "iteration:1563 | d_loss:-0.028 | g_loss:-0.386\n",
      "iteration:1564 | d_loss:-0.028 | g_loss:-0.386\n",
      "iteration:1565 | d_loss:-0.029 | g_loss:-0.386\n",
      "iteration:1566 | d_loss:-0.029 | g_loss:-0.386\n",
      "iteration:1567 | d_loss:-0.029 | g_loss:-0.386\n",
      "iteration:1568 | d_loss:-0.029 | g_loss:-0.386\n",
      "iteration:1569 | d_loss:-0.029 | g_loss:-0.385\n",
      "iteration:1570 | d_loss:-0.030 | g_loss:-0.385\n",
      "iteration:1571 | d_loss:-0.030 | g_loss:-0.385\n",
      "iteration:1572 | d_loss:-0.030 | g_loss:-0.385\n",
      "iteration:1573 | d_loss:-0.031 | g_loss:-0.385\n",
      "iteration:1574 | d_loss:-0.030 | g_loss:-0.385\n",
      "iteration:1575 | d_loss:-0.031 | g_loss:-0.384\n",
      "iteration:1576 | d_loss:-0.031 | g_loss:-0.384\n",
      "iteration:1577 | d_loss:-0.031 | g_loss:-0.384\n",
      "iteration:1578 | d_loss:-0.032 | g_loss:-0.384\n",
      "iteration:1579 | d_loss:-0.031 | g_loss:-0.384\n",
      "iteration:1580 | d_loss:-0.032 | g_loss:-0.384\n",
      "iteration:1581 | d_loss:-0.033 | g_loss:-0.384\n",
      "iteration:1582 | d_loss:-0.033 | g_loss:-0.383\n",
      "iteration:1583 | d_loss:-0.033 | g_loss:-0.383\n",
      "iteration:1584 | d_loss:-0.033 | g_loss:-0.383\n",
      "iteration:1585 | d_loss:-0.033 | g_loss:-0.383\n",
      "iteration:1586 | d_loss:-0.033 | g_loss:-0.383\n",
      "iteration:1587 | d_loss:-0.034 | g_loss:-0.382\n",
      "iteration:1588 | d_loss:-0.034 | g_loss:-0.382\n",
      "iteration:1589 | d_loss:-0.035 | g_loss:-0.382\n",
      "iteration:1590 | d_loss:-0.034 | g_loss:-0.382\n",
      "iteration:1591 | d_loss:-0.035 | g_loss:-0.382\n",
      "iteration:1592 | d_loss:-0.035 | g_loss:-0.382\n",
      "iteration:1593 | d_loss:-0.035 | g_loss:-0.381\n",
      "iteration:1594 | d_loss:-0.035 | g_loss:-0.381\n",
      "iteration:1595 | d_loss:-0.035 | g_loss:-0.381\n",
      "iteration:1596 | d_loss:-0.036 | g_loss:-0.381\n",
      "iteration:1597 | d_loss:-0.036 | g_loss:-0.381\n",
      "iteration:1598 | d_loss:-0.036 | g_loss:-0.380\n",
      "iteration:1599 | d_loss:-0.037 | g_loss:-0.380\n",
      "iteration:1600 | d_loss:-0.037 | g_loss:-0.380\n",
      "iteration:1601 | d_loss:-0.037 | g_loss:-0.380\n",
      "iteration:1602 | d_loss:-0.038 | g_loss:-0.380\n",
      "iteration:1603 | d_loss:-0.038 | g_loss:-0.380\n",
      "iteration:1604 | d_loss:-0.037 | g_loss:-0.380\n",
      "iteration:1605 | d_loss:-0.038 | g_loss:-0.379\n",
      "iteration:1606 | d_loss:-0.039 | g_loss:-0.379\n",
      "iteration:1607 | d_loss:-0.039 | g_loss:-0.379\n",
      "iteration:1608 | d_loss:-0.038 | g_loss:-0.379\n",
      "iteration:1609 | d_loss:-0.039 | g_loss:-0.379\n",
      "iteration:1610 | d_loss:-0.039 | g_loss:-0.378\n",
      "iteration:1611 | d_loss:-0.039 | g_loss:-0.378\n",
      "iteration:1612 | d_loss:-0.040 | g_loss:-0.378\n",
      "iteration:1613 | d_loss:-0.040 | g_loss:-0.378\n",
      "iteration:1614 | d_loss:-0.041 | g_loss:-0.378\n",
      "iteration:1615 | d_loss:-0.041 | g_loss:-0.378\n",
      "iteration:1616 | d_loss:-0.041 | g_loss:-0.377\n",
      "iteration:1617 | d_loss:-0.041 | g_loss:-0.377\n",
      "iteration:1618 | d_loss:-0.042 | g_loss:-0.377\n",
      "iteration:1619 | d_loss:-0.042 | g_loss:-0.377\n",
      "iteration:1620 | d_loss:-0.042 | g_loss:-0.376\n",
      "iteration:1621 | d_loss:-0.042 | g_loss:-0.376\n",
      "iteration:1622 | d_loss:-0.042 | g_loss:-0.376\n",
      "iteration:1623 | d_loss:-0.043 | g_loss:-0.376\n",
      "iteration:1624 | d_loss:-0.043 | g_loss:-0.376\n",
      "iteration:1625 | d_loss:-0.044 | g_loss:-0.375\n",
      "iteration:1626 | d_loss:-0.044 | g_loss:-0.375\n",
      "iteration:1627 | d_loss:-0.043 | g_loss:-0.375\n",
      "iteration:1628 | d_loss:-0.045 | g_loss:-0.375\n",
      "iteration:1629 | d_loss:-0.045 | g_loss:-0.374\n",
      "iteration:1630 | d_loss:-0.044 | g_loss:-0.374\n",
      "iteration:1631 | d_loss:-0.045 | g_loss:-0.374\n",
      "iteration:1632 | d_loss:-0.045 | g_loss:-0.374\n",
      "iteration:1633 | d_loss:-0.045 | g_loss:-0.373\n",
      "iteration:1634 | d_loss:-0.046 | g_loss:-0.373\n",
      "iteration:1635 | d_loss:-0.046 | g_loss:-0.373\n",
      "iteration:1636 | d_loss:-0.046 | g_loss:-0.373\n",
      "iteration:1637 | d_loss:-0.047 | g_loss:-0.372\n",
      "iteration:1638 | d_loss:-0.047 | g_loss:-0.372\n",
      "iteration:1639 | d_loss:-0.047 | g_loss:-0.372\n",
      "iteration:1640 | d_loss:-0.047 | g_loss:-0.372\n",
      "iteration:1641 | d_loss:-0.047 | g_loss:-0.372\n",
      "iteration:1642 | d_loss:-0.048 | g_loss:-0.372\n",
      "iteration:1643 | d_loss:-0.048 | g_loss:-0.372\n",
      "iteration:1644 | d_loss:-0.049 | g_loss:-0.371\n",
      "iteration:1645 | d_loss:-0.048 | g_loss:-0.371\n",
      "iteration:1646 | d_loss:-0.048 | g_loss:-0.371\n",
      "iteration:1647 | d_loss:-0.049 | g_loss:-0.371\n",
      "iteration:1648 | d_loss:-0.049 | g_loss:-0.370\n",
      "iteration:1649 | d_loss:-0.050 | g_loss:-0.370\n",
      "iteration:1650 | d_loss:-0.050 | g_loss:-0.370\n",
      "iteration:1651 | d_loss:-0.050 | g_loss:-0.370\n",
      "iteration:1652 | d_loss:-0.050 | g_loss:-0.370\n",
      "iteration:1653 | d_loss:-0.051 | g_loss:-0.369\n",
      "iteration:1654 | d_loss:-0.051 | g_loss:-0.369\n",
      "iteration:1655 | d_loss:-0.051 | g_loss:-0.369\n",
      "iteration:1656 | d_loss:-0.051 | g_loss:-0.369\n",
      "iteration:1657 | d_loss:-0.051 | g_loss:-0.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1658 | d_loss:-0.051 | g_loss:-0.369\n",
      "iteration:1659 | d_loss:-0.052 | g_loss:-0.368\n",
      "iteration:1660 | d_loss:-0.052 | g_loss:-0.369\n",
      "iteration:1661 | d_loss:-0.052 | g_loss:-0.368\n",
      "iteration:1662 | d_loss:-0.053 | g_loss:-0.368\n",
      "iteration:1663 | d_loss:-0.052 | g_loss:-0.368\n",
      "iteration:1664 | d_loss:-0.053 | g_loss:-0.367\n",
      "iteration:1665 | d_loss:-0.053 | g_loss:-0.368\n",
      "iteration:1666 | d_loss:-0.054 | g_loss:-0.367\n",
      "iteration:1667 | d_loss:-0.054 | g_loss:-0.367\n",
      "iteration:1668 | d_loss:-0.054 | g_loss:-0.367\n",
      "iteration:1669 | d_loss:-0.054 | g_loss:-0.366\n",
      "iteration:1670 | d_loss:-0.054 | g_loss:-0.366\n",
      "iteration:1671 | d_loss:-0.054 | g_loss:-0.366\n",
      "iteration:1672 | d_loss:-0.055 | g_loss:-0.366\n",
      "iteration:1673 | d_loss:-0.055 | g_loss:-0.366\n",
      "iteration:1674 | d_loss:-0.055 | g_loss:-0.366\n",
      "iteration:1675 | d_loss:-0.055 | g_loss:-0.366\n",
      "iteration:1676 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1677 | d_loss:-0.056 | g_loss:-0.366\n",
      "iteration:1678 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1679 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1680 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1681 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1682 | d_loss:-0.056 | g_loss:-0.365\n",
      "iteration:1683 | d_loss:-0.056 | g_loss:-0.364\n",
      "iteration:1684 | d_loss:-0.057 | g_loss:-0.364\n",
      "iteration:1685 | d_loss:-0.057 | g_loss:-0.364\n",
      "iteration:1686 | d_loss:-0.058 | g_loss:-0.364\n",
      "iteration:1687 | d_loss:-0.058 | g_loss:-0.364\n",
      "iteration:1688 | d_loss:-0.058 | g_loss:-0.364\n",
      "iteration:1689 | d_loss:-0.058 | g_loss:-0.364\n",
      "iteration:1690 | d_loss:-0.058 | g_loss:-0.363\n",
      "iteration:1691 | d_loss:-0.058 | g_loss:-0.364\n",
      "iteration:1692 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1693 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1694 | d_loss:-0.059 | g_loss:-0.362\n",
      "iteration:1695 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1696 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1697 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1698 | d_loss:-0.059 | g_loss:-0.363\n",
      "iteration:1699 | d_loss:-0.059 | g_loss:-0.362\n",
      "iteration:1700 | d_loss:-0.060 | g_loss:-0.363\n",
      "iteration:1701 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1702 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1703 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1704 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1705 | d_loss:-0.061 | g_loss:-0.362\n",
      "iteration:1706 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1707 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1708 | d_loss:-0.060 | g_loss:-0.362\n",
      "iteration:1709 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1710 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1711 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1712 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1713 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1714 | d_loss:-0.061 | g_loss:-0.361\n",
      "iteration:1715 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1716 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1717 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1718 | d_loss:-0.062 | g_loss:-0.360\n",
      "iteration:1719 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1720 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1721 | d_loss:-0.062 | g_loss:-0.360\n",
      "iteration:1722 | d_loss:-0.062 | g_loss:-0.361\n",
      "iteration:1723 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1724 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1725 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1726 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1727 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1728 | d_loss:-0.062 | g_loss:-0.360\n",
      "iteration:1729 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1730 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1731 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1732 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1733 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1734 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1735 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1736 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1737 | d_loss:-0.064 | g_loss:-0.359\n",
      "iteration:1738 | d_loss:-0.064 | g_loss:-0.359\n",
      "iteration:1739 | d_loss:-0.063 | g_loss:-0.359\n",
      "iteration:1740 | d_loss:-0.064 | g_loss:-0.359\n",
      "iteration:1741 | d_loss:-0.065 | g_loss:-0.359\n",
      "iteration:1742 | d_loss:-0.065 | g_loss:-0.360\n",
      "iteration:1743 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1744 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1745 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1746 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1747 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1748 | d_loss:-0.065 | g_loss:-0.360\n",
      "iteration:1749 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1750 | d_loss:-0.064 | g_loss:-0.359\n",
      "iteration:1751 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1752 | d_loss:-0.064 | g_loss:-0.361\n",
      "iteration:1753 | d_loss:-0.064 | g_loss:-0.359\n",
      "iteration:1754 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1755 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1756 | d_loss:-0.064 | g_loss:-0.361\n",
      "iteration:1757 | d_loss:-0.065 | g_loss:-0.361\n",
      "iteration:1758 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1759 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1760 | d_loss:-0.065 | g_loss:-0.360\n",
      "iteration:1761 | d_loss:-0.065 | g_loss:-0.360\n",
      "iteration:1762 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1763 | d_loss:-0.063 | g_loss:-0.360\n",
      "iteration:1764 | d_loss:-0.064 | g_loss:-0.360\n",
      "iteration:1765 | d_loss:-0.063 | g_loss:-0.361\n",
      "iteration:1766 | d_loss:-0.063 | g_loss:-0.362\n",
      "iteration:1767 | d_loss:-0.063 | g_loss:-0.361\n",
      "iteration:1768 | d_loss:-0.063 | g_loss:-0.361\n",
      "iteration:1769 | d_loss:-0.064 | g_loss:-0.362\n",
      "iteration:1770 | d_loss:-0.064 | g_loss:-0.362\n",
      "iteration:1771 | d_loss:-0.064 | g_loss:-0.362\n",
      "iteration:1772 | d_loss:-0.062 | g_loss:-0.362\n",
      "iteration:1773 | d_loss:-0.064 | g_loss:-0.363\n",
      "iteration:1774 | d_loss:-0.063 | g_loss:-0.362\n",
      "iteration:1775 | d_loss:-0.063 | g_loss:-0.363\n",
      "iteration:1776 | d_loss:-0.062 | g_loss:-0.363\n",
      "iteration:1777 | d_loss:-0.062 | g_loss:-0.363\n",
      "iteration:1778 | d_loss:-0.061 | g_loss:-0.363\n",
      "iteration:1779 | d_loss:-0.061 | g_loss:-0.363\n",
      "iteration:1780 | d_loss:-0.061 | g_loss:-0.364\n",
      "iteration:1781 | d_loss:-0.060 | g_loss:-0.364\n",
      "iteration:1782 | d_loss:-0.060 | g_loss:-0.364\n",
      "iteration:1783 | d_loss:-0.060 | g_loss:-0.365\n",
      "iteration:1784 | d_loss:-0.060 | g_loss:-0.365\n",
      "iteration:1785 | d_loss:-0.059 | g_loss:-0.366\n",
      "iteration:1786 | d_loss:-0.059 | g_loss:-0.365\n",
      "iteration:1787 | d_loss:-0.058 | g_loss:-0.366\n",
      "iteration:1788 | d_loss:-0.059 | g_loss:-0.367\n",
      "iteration:1789 | d_loss:-0.058 | g_loss:-0.367\n",
      "iteration:1790 | d_loss:-0.058 | g_loss:-0.367\n",
      "iteration:1791 | d_loss:-0.059 | g_loss:-0.366\n",
      "iteration:1792 | d_loss:-0.056 | g_loss:-0.367\n",
      "iteration:1793 | d_loss:-0.057 | g_loss:-0.368\n",
      "iteration:1794 | d_loss:-0.057 | g_loss:-0.368\n",
      "iteration:1795 | d_loss:-0.056 | g_loss:-0.369\n",
      "iteration:1796 | d_loss:-0.055 | g_loss:-0.370\n",
      "iteration:1797 | d_loss:-0.055 | g_loss:-0.369\n",
      "iteration:1798 | d_loss:-0.055 | g_loss:-0.369\n",
      "iteration:1799 | d_loss:-0.054 | g_loss:-0.371\n",
      "iteration:1800 | d_loss:-0.054 | g_loss:-0.371\n",
      "iteration:1801 | d_loss:-0.051 | g_loss:-0.370\n",
      "iteration:1802 | d_loss:-0.052 | g_loss:-0.371\n",
      "iteration:1803 | d_loss:-0.052 | g_loss:-0.373\n",
      "iteration:1804 | d_loss:-0.052 | g_loss:-0.372\n",
      "iteration:1805 | d_loss:-0.052 | g_loss:-0.373\n",
      "iteration:1806 | d_loss:-0.051 | g_loss:-0.373\n",
      "iteration:1807 | d_loss:-0.049 | g_loss:-0.373\n",
      "iteration:1808 | d_loss:-0.049 | g_loss:-0.375\n",
      "iteration:1809 | d_loss:-0.049 | g_loss:-0.376\n",
      "iteration:1810 | d_loss:-0.048 | g_loss:-0.376\n",
      "iteration:1811 | d_loss:-0.048 | g_loss:-0.376\n",
      "iteration:1812 | d_loss:-0.047 | g_loss:-0.377\n",
      "iteration:1813 | d_loss:-0.046 | g_loss:-0.378\n",
      "iteration:1814 | d_loss:-0.047 | g_loss:-0.378\n",
      "iteration:1815 | d_loss:-0.046 | g_loss:-0.379\n",
      "iteration:1816 | d_loss:-0.044 | g_loss:-0.379\n",
      "iteration:1817 | d_loss:-0.044 | g_loss:-0.380\n",
      "iteration:1818 | d_loss:-0.042 | g_loss:-0.381\n",
      "iteration:1819 | d_loss:-0.043 | g_loss:-0.382\n",
      "iteration:1820 | d_loss:-0.041 | g_loss:-0.382\n",
      "iteration:1821 | d_loss:-0.041 | g_loss:-0.383\n",
      "iteration:1822 | d_loss:-0.040 | g_loss:-0.382\n",
      "iteration:1823 | d_loss:-0.040 | g_loss:-0.383\n",
      "iteration:1824 | d_loss:-0.038 | g_loss:-0.384\n",
      "iteration:1825 | d_loss:-0.036 | g_loss:-0.384\n",
      "iteration:1826 | d_loss:-0.038 | g_loss:-0.386\n",
      "iteration:1827 | d_loss:-0.037 | g_loss:-0.387\n",
      "iteration:1828 | d_loss:-0.037 | g_loss:-0.386\n",
      "iteration:1829 | d_loss:-0.035 | g_loss:-0.388\n",
      "iteration:1830 | d_loss:-0.033 | g_loss:-0.389\n",
      "iteration:1831 | d_loss:-0.035 | g_loss:-0.389\n",
      "iteration:1832 | d_loss:-0.032 | g_loss:-0.389\n",
      "iteration:1833 | d_loss:-0.033 | g_loss:-0.390\n",
      "iteration:1834 | d_loss:-0.032 | g_loss:-0.391\n",
      "iteration:1835 | d_loss:-0.031 | g_loss:-0.392\n",
      "iteration:1836 | d_loss:-0.032 | g_loss:-0.393\n",
      "iteration:1837 | d_loss:-0.028 | g_loss:-0.392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1838 | d_loss:-0.027 | g_loss:-0.392\n",
      "iteration:1839 | d_loss:-0.028 | g_loss:-0.394\n",
      "iteration:1840 | d_loss:-0.027 | g_loss:-0.395\n",
      "iteration:1841 | d_loss:-0.026 | g_loss:-0.396\n",
      "iteration:1842 | d_loss:-0.025 | g_loss:-0.397\n",
      "iteration:1843 | d_loss:-0.022 | g_loss:-0.396\n",
      "iteration:1844 | d_loss:-0.023 | g_loss:-0.397\n",
      "iteration:1845 | d_loss:-0.022 | g_loss:-0.398\n",
      "iteration:1846 | d_loss:-0.022 | g_loss:-0.399\n",
      "iteration:1847 | d_loss:-0.019 | g_loss:-0.400\n",
      "iteration:1848 | d_loss:-0.020 | g_loss:-0.399\n",
      "iteration:1849 | d_loss:-0.020 | g_loss:-0.400\n",
      "iteration:1850 | d_loss:-0.019 | g_loss:-0.402\n",
      "iteration:1851 | d_loss:-0.018 | g_loss:-0.402\n",
      "iteration:1852 | d_loss:-0.018 | g_loss:-0.403\n",
      "iteration:1853 | d_loss:-0.016 | g_loss:-0.403\n",
      "iteration:1854 | d_loss:-0.015 | g_loss:-0.403\n",
      "iteration:1855 | d_loss:-0.014 | g_loss:-0.404\n",
      "iteration:1856 | d_loss:-0.015 | g_loss:-0.405\n",
      "iteration:1857 | d_loss:-0.013 | g_loss:-0.406\n",
      "iteration:1858 | d_loss:-0.014 | g_loss:-0.405\n",
      "iteration:1859 | d_loss:-0.012 | g_loss:-0.406\n",
      "iteration:1860 | d_loss:-0.011 | g_loss:-0.407\n",
      "iteration:1861 | d_loss:-0.011 | g_loss:-0.407\n",
      "iteration:1862 | d_loss:-0.009 | g_loss:-0.408\n",
      "iteration:1863 | d_loss:-0.009 | g_loss:-0.408\n",
      "iteration:1864 | d_loss:-0.010 | g_loss:-0.409\n",
      "iteration:1865 | d_loss:-0.008 | g_loss:-0.410\n",
      "iteration:1866 | d_loss:-0.008 | g_loss:-0.410\n",
      "iteration:1867 | d_loss:-0.007 | g_loss:-0.411\n",
      "iteration:1868 | d_loss:-0.006 | g_loss:-0.412\n",
      "iteration:1869 | d_loss:-0.004 | g_loss:-0.412\n",
      "iteration:1870 | d_loss:-0.004 | g_loss:-0.413\n",
      "iteration:1871 | d_loss:-0.003 | g_loss:-0.413\n",
      "iteration:1872 | d_loss:-0.004 | g_loss:-0.414\n",
      "iteration:1873 | d_loss:-0.002 | g_loss:-0.414\n",
      "iteration:1874 | d_loss:-0.001 | g_loss:-0.415\n",
      "iteration:1875 | d_loss:-0.001 | g_loss:-0.416\n",
      "iteration:1876 | d_loss:0.000 | g_loss:-0.416\n",
      "iteration:1877 | d_loss:-0.001 | g_loss:-0.417\n",
      "iteration:1878 | d_loss:0.000 | g_loss:-0.417\n",
      "iteration:1879 | d_loss:0.002 | g_loss:-0.417\n",
      "iteration:1880 | d_loss:0.001 | g_loss:-0.417\n",
      "iteration:1881 | d_loss:0.001 | g_loss:-0.418\n",
      "iteration:1882 | d_loss:0.002 | g_loss:-0.419\n",
      "iteration:1883 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1884 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1885 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1886 | d_loss:0.002 | g_loss:-0.419\n",
      "iteration:1887 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1888 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1889 | d_loss:0.002 | g_loss:-0.420\n",
      "iteration:1890 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1891 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1892 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1893 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1894 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1895 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1896 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1897 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1898 | d_loss:0.003 | g_loss:-0.421\n",
      "iteration:1899 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1900 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1901 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1902 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1903 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:1904 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1905 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1906 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1907 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1908 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1909 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1910 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1911 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1912 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1913 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1914 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1915 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:1916 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:1917 | d_loss:0.006 | g_loss:-0.420\n",
      "iteration:1918 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:1919 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:1920 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1921 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1922 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1923 | d_loss:0.006 | g_loss:-0.422\n",
      "iteration:1924 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1925 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:1926 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:1927 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1928 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1929 | d_loss:0.006 | g_loss:-0.420\n",
      "iteration:1930 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1931 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:1932 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:1933 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1934 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1935 | d_loss:0.006 | g_loss:-0.420\n",
      "iteration:1936 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1937 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1938 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1939 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1940 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:1941 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1942 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1943 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1944 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1945 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1946 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1947 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1948 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1949 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1950 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1951 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1952 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1953 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:1954 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1955 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1956 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1957 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1958 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1959 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1960 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1961 | d_loss:0.003 | g_loss:-0.420\n",
      "iteration:1962 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1963 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1964 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1965 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1966 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1967 | d_loss:0.005 | g_loss:-0.419\n",
      "iteration:1968 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1969 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1970 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1971 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1972 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1973 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1974 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1975 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1976 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1977 | d_loss:0.002 | g_loss:-0.418\n",
      "iteration:1978 | d_loss:0.002 | g_loss:-0.419\n",
      "iteration:1979 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1980 | d_loss:0.002 | g_loss:-0.418\n",
      "iteration:1981 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1982 | d_loss:0.004 | g_loss:-0.418\n",
      "iteration:1983 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1984 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1985 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1986 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1987 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1988 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1989 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1990 | d_loss:0.001 | g_loss:-0.418\n",
      "iteration:1991 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1992 | d_loss:0.003 | g_loss:-0.418\n",
      "iteration:1993 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1994 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1995 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:1996 | d_loss:0.003 | g_loss:-0.419\n",
      "iteration:1997 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1998 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:1999 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:2000 | d_loss:0.004 | g_loss:-0.420\n",
      "iteration:2001 | d_loss:0.004 | g_loss:-0.419\n",
      "iteration:2002 | d_loss:0.005 | g_loss:-0.420\n",
      "iteration:2003 | d_loss:0.005 | g_loss:-0.419\n",
      "iteration:2004 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:2005 | d_loss:0.003 | g_loss:-0.421\n",
      "iteration:2006 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:2007 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:2008 | d_loss:0.004 | g_loss:-0.421\n",
      "iteration:2009 | d_loss:0.005 | g_loss:-0.422\n",
      "iteration:2010 | d_loss:0.005 | g_loss:-0.421\n",
      "iteration:2011 | d_loss:0.006 | g_loss:-0.421\n",
      "iteration:2012 | d_loss:0.006 | g_loss:-0.422\n",
      "iteration:2013 | d_loss:0.005 | g_loss:-0.423\n",
      "iteration:2014 | d_loss:0.005 | g_loss:-0.422\n",
      "iteration:2015 | d_loss:0.007 | g_loss:-0.422\n",
      "iteration:2016 | d_loss:0.005 | g_loss:-0.422\n",
      "iteration:2017 | d_loss:0.006 | g_loss:-0.422\n",
      "iteration:2018 | d_loss:0.007 | g_loss:-0.421\n",
      "iteration:2019 | d_loss:0.006 | g_loss:-0.422\n",
      "iteration:2020 | d_loss:0.007 | g_loss:-0.422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2021 | d_loss:0.007 | g_loss:-0.422\n",
      "iteration:2022 | d_loss:0.008 | g_loss:-0.421\n",
      "iteration:2023 | d_loss:0.008 | g_loss:-0.422\n",
      "iteration:2024 | d_loss:0.008 | g_loss:-0.422\n",
      "iteration:2025 | d_loss:0.008 | g_loss:-0.422\n",
      "iteration:2026 | d_loss:0.008 | g_loss:-0.423\n",
      "iteration:2027 | d_loss:0.008 | g_loss:-0.422\n",
      "iteration:2028 | d_loss:0.009 | g_loss:-0.422\n",
      "iteration:2029 | d_loss:0.009 | g_loss:-0.422\n",
      "iteration:2030 | d_loss:0.009 | g_loss:-0.422\n",
      "iteration:2031 | d_loss:0.010 | g_loss:-0.423\n",
      "iteration:2032 | d_loss:0.009 | g_loss:-0.422\n",
      "iteration:2033 | d_loss:0.010 | g_loss:-0.423\n",
      "iteration:2034 | d_loss:0.010 | g_loss:-0.422\n",
      "iteration:2035 | d_loss:0.010 | g_loss:-0.423\n",
      "iteration:2036 | d_loss:0.011 | g_loss:-0.423\n",
      "iteration:2037 | d_loss:0.009 | g_loss:-0.423\n",
      "iteration:2038 | d_loss:0.012 | g_loss:-0.423\n",
      "iteration:2039 | d_loss:0.012 | g_loss:-0.423\n",
      "iteration:2040 | d_loss:0.012 | g_loss:-0.423\n",
      "iteration:2041 | d_loss:0.013 | g_loss:-0.423\n",
      "iteration:2042 | d_loss:0.015 | g_loss:-0.423\n",
      "iteration:2043 | d_loss:0.013 | g_loss:-0.424\n",
      "iteration:2044 | d_loss:0.014 | g_loss:-0.424\n",
      "iteration:2045 | d_loss:0.014 | g_loss:-0.424\n",
      "iteration:2046 | d_loss:0.014 | g_loss:-0.424\n",
      "iteration:2047 | d_loss:0.014 | g_loss:-0.423\n",
      "iteration:2048 | d_loss:0.014 | g_loss:-0.425\n",
      "iteration:2049 | d_loss:0.014 | g_loss:-0.425\n",
      "iteration:2050 | d_loss:0.015 | g_loss:-0.425\n",
      "iteration:2051 | d_loss:0.014 | g_loss:-0.425\n",
      "iteration:2052 | d_loss:0.015 | g_loss:-0.425\n",
      "iteration:2053 | d_loss:0.016 | g_loss:-0.425\n",
      "iteration:2054 | d_loss:0.016 | g_loss:-0.425\n",
      "iteration:2055 | d_loss:0.017 | g_loss:-0.425\n",
      "iteration:2056 | d_loss:0.016 | g_loss:-0.425\n",
      "iteration:2057 | d_loss:0.017 | g_loss:-0.425\n",
      "iteration:2058 | d_loss:0.017 | g_loss:-0.425\n",
      "iteration:2059 | d_loss:0.018 | g_loss:-0.425\n",
      "iteration:2060 | d_loss:0.018 | g_loss:-0.426\n",
      "iteration:2061 | d_loss:0.019 | g_loss:-0.426\n",
      "iteration:2062 | d_loss:0.018 | g_loss:-0.426\n",
      "iteration:2063 | d_loss:0.019 | g_loss:-0.426\n",
      "iteration:2064 | d_loss:0.019 | g_loss:-0.426\n",
      "iteration:2065 | d_loss:0.019 | g_loss:-0.426\n",
      "iteration:2066 | d_loss:0.022 | g_loss:-0.426\n",
      "iteration:2067 | d_loss:0.020 | g_loss:-0.428\n",
      "iteration:2068 | d_loss:0.020 | g_loss:-0.427\n",
      "iteration:2069 | d_loss:0.021 | g_loss:-0.427\n",
      "iteration:2070 | d_loss:0.021 | g_loss:-0.428\n",
      "iteration:2071 | d_loss:0.021 | g_loss:-0.428\n",
      "iteration:2072 | d_loss:0.023 | g_loss:-0.428\n",
      "iteration:2073 | d_loss:0.023 | g_loss:-0.429\n",
      "iteration:2074 | d_loss:0.022 | g_loss:-0.429\n",
      "iteration:2075 | d_loss:0.020 | g_loss:-0.428\n",
      "iteration:2076 | d_loss:0.024 | g_loss:-0.429\n",
      "iteration:2077 | d_loss:0.023 | g_loss:-0.429\n",
      "iteration:2078 | d_loss:0.024 | g_loss:-0.430\n",
      "iteration:2079 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:2080 | d_loss:0.026 | g_loss:-0.431\n",
      "iteration:2081 | d_loss:0.025 | g_loss:-0.430\n",
      "iteration:2082 | d_loss:0.026 | g_loss:-0.431\n",
      "iteration:2083 | d_loss:0.026 | g_loss:-0.431\n",
      "iteration:2084 | d_loss:0.026 | g_loss:-0.430\n",
      "iteration:2085 | d_loss:0.026 | g_loss:-0.432\n",
      "iteration:2086 | d_loss:0.027 | g_loss:-0.431\n",
      "iteration:2087 | d_loss:0.028 | g_loss:-0.432\n",
      "iteration:2088 | d_loss:0.030 | g_loss:-0.433\n",
      "iteration:2089 | d_loss:0.030 | g_loss:-0.432\n",
      "iteration:2090 | d_loss:0.029 | g_loss:-0.434\n",
      "iteration:2091 | d_loss:0.031 | g_loss:-0.433\n",
      "iteration:2092 | d_loss:0.030 | g_loss:-0.433\n",
      "iteration:2093 | d_loss:0.031 | g_loss:-0.434\n",
      "iteration:2094 | d_loss:0.031 | g_loss:-0.434\n",
      "iteration:2095 | d_loss:0.032 | g_loss:-0.434\n",
      "iteration:2096 | d_loss:0.033 | g_loss:-0.434\n",
      "iteration:2097 | d_loss:0.033 | g_loss:-0.435\n",
      "iteration:2098 | d_loss:0.032 | g_loss:-0.435\n",
      "iteration:2099 | d_loss:0.033 | g_loss:-0.435\n",
      "iteration:2100 | d_loss:0.034 | g_loss:-0.436\n",
      "iteration:2101 | d_loss:0.035 | g_loss:-0.435\n",
      "iteration:2102 | d_loss:0.034 | g_loss:-0.436\n",
      "iteration:2103 | d_loss:0.035 | g_loss:-0.436\n",
      "iteration:2104 | d_loss:0.036 | g_loss:-0.436\n",
      "iteration:2105 | d_loss:0.037 | g_loss:-0.437\n",
      "iteration:2106 | d_loss:0.037 | g_loss:-0.437\n",
      "iteration:2107 | d_loss:0.036 | g_loss:-0.437\n",
      "iteration:2108 | d_loss:0.037 | g_loss:-0.437\n",
      "iteration:2109 | d_loss:0.037 | g_loss:-0.437\n",
      "iteration:2110 | d_loss:0.038 | g_loss:-0.437\n",
      "iteration:2111 | d_loss:0.039 | g_loss:-0.438\n",
      "iteration:2112 | d_loss:0.037 | g_loss:-0.439\n",
      "iteration:2113 | d_loss:0.038 | g_loss:-0.439\n",
      "iteration:2114 | d_loss:0.038 | g_loss:-0.439\n",
      "iteration:2115 | d_loss:0.042 | g_loss:-0.439\n",
      "iteration:2116 | d_loss:0.041 | g_loss:-0.440\n",
      "iteration:2117 | d_loss:0.041 | g_loss:-0.440\n",
      "iteration:2118 | d_loss:0.041 | g_loss:-0.441\n",
      "iteration:2119 | d_loss:0.042 | g_loss:-0.440\n",
      "iteration:2120 | d_loss:0.042 | g_loss:-0.440\n",
      "iteration:2121 | d_loss:0.041 | g_loss:-0.441\n",
      "iteration:2122 | d_loss:0.043 | g_loss:-0.442\n",
      "iteration:2123 | d_loss:0.042 | g_loss:-0.441\n",
      "iteration:2124 | d_loss:0.041 | g_loss:-0.442\n",
      "iteration:2125 | d_loss:0.044 | g_loss:-0.442\n",
      "iteration:2126 | d_loss:0.044 | g_loss:-0.443\n",
      "iteration:2127 | d_loss:0.041 | g_loss:-0.443\n",
      "iteration:2128 | d_loss:0.043 | g_loss:-0.443\n",
      "iteration:2129 | d_loss:0.044 | g_loss:-0.443\n",
      "iteration:2130 | d_loss:0.043 | g_loss:-0.443\n",
      "iteration:2131 | d_loss:0.044 | g_loss:-0.443\n",
      "iteration:2132 | d_loss:0.044 | g_loss:-0.443\n",
      "iteration:2133 | d_loss:0.044 | g_loss:-0.444\n",
      "iteration:2134 | d_loss:0.043 | g_loss:-0.445\n",
      "iteration:2135 | d_loss:0.045 | g_loss:-0.445\n",
      "iteration:2136 | d_loss:0.045 | g_loss:-0.444\n",
      "iteration:2137 | d_loss:0.046 | g_loss:-0.445\n",
      "iteration:2138 | d_loss:0.046 | g_loss:-0.446\n",
      "iteration:2139 | d_loss:0.046 | g_loss:-0.446\n",
      "iteration:2140 | d_loss:0.047 | g_loss:-0.447\n",
      "iteration:2141 | d_loss:0.046 | g_loss:-0.446\n",
      "iteration:2142 | d_loss:0.047 | g_loss:-0.446\n",
      "iteration:2143 | d_loss:0.048 | g_loss:-0.447\n",
      "iteration:2144 | d_loss:0.047 | g_loss:-0.448\n",
      "iteration:2145 | d_loss:0.048 | g_loss:-0.448\n",
      "iteration:2146 | d_loss:0.048 | g_loss:-0.448\n",
      "iteration:2147 | d_loss:0.049 | g_loss:-0.448\n",
      "iteration:2148 | d_loss:0.049 | g_loss:-0.449\n",
      "iteration:2149 | d_loss:0.049 | g_loss:-0.449\n",
      "iteration:2150 | d_loss:0.049 | g_loss:-0.450\n",
      "iteration:2151 | d_loss:0.048 | g_loss:-0.449\n",
      "iteration:2152 | d_loss:0.049 | g_loss:-0.450\n",
      "iteration:2153 | d_loss:0.049 | g_loss:-0.450\n",
      "iteration:2154 | d_loss:0.050 | g_loss:-0.450\n",
      "iteration:2155 | d_loss:0.050 | g_loss:-0.451\n",
      "iteration:2156 | d_loss:0.049 | g_loss:-0.451\n",
      "iteration:2157 | d_loss:0.051 | g_loss:-0.451\n",
      "iteration:2158 | d_loss:0.050 | g_loss:-0.451\n",
      "iteration:2159 | d_loss:0.051 | g_loss:-0.451\n",
      "iteration:2160 | d_loss:0.051 | g_loss:-0.452\n",
      "iteration:2161 | d_loss:0.053 | g_loss:-0.451\n",
      "iteration:2162 | d_loss:0.052 | g_loss:-0.453\n",
      "iteration:2163 | d_loss:0.052 | g_loss:-0.452\n",
      "iteration:2164 | d_loss:0.052 | g_loss:-0.453\n",
      "iteration:2165 | d_loss:0.052 | g_loss:-0.453\n",
      "iteration:2166 | d_loss:0.052 | g_loss:-0.453\n",
      "iteration:2167 | d_loss:0.053 | g_loss:-0.453\n",
      "iteration:2168 | d_loss:0.053 | g_loss:-0.454\n",
      "iteration:2169 | d_loss:0.054 | g_loss:-0.454\n",
      "iteration:2170 | d_loss:0.052 | g_loss:-0.454\n",
      "iteration:2171 | d_loss:0.052 | g_loss:-0.454\n",
      "iteration:2172 | d_loss:0.052 | g_loss:-0.455\n",
      "iteration:2173 | d_loss:0.053 | g_loss:-0.455\n",
      "iteration:2174 | d_loss:0.054 | g_loss:-0.455\n",
      "iteration:2175 | d_loss:0.053 | g_loss:-0.456\n",
      "iteration:2176 | d_loss:0.053 | g_loss:-0.456\n",
      "iteration:2177 | d_loss:0.053 | g_loss:-0.456\n",
      "iteration:2178 | d_loss:0.054 | g_loss:-0.456\n",
      "iteration:2179 | d_loss:0.054 | g_loss:-0.457\n",
      "iteration:2180 | d_loss:0.054 | g_loss:-0.457\n",
      "iteration:2181 | d_loss:0.054 | g_loss:-0.457\n",
      "iteration:2182 | d_loss:0.055 | g_loss:-0.458\n",
      "iteration:2183 | d_loss:0.055 | g_loss:-0.459\n",
      "iteration:2184 | d_loss:0.055 | g_loss:-0.459\n",
      "iteration:2185 | d_loss:0.055 | g_loss:-0.458\n",
      "iteration:2186 | d_loss:0.054 | g_loss:-0.459\n",
      "iteration:2187 | d_loss:0.055 | g_loss:-0.459\n",
      "iteration:2188 | d_loss:0.054 | g_loss:-0.460\n",
      "iteration:2189 | d_loss:0.053 | g_loss:-0.460\n",
      "iteration:2190 | d_loss:0.055 | g_loss:-0.461\n",
      "iteration:2191 | d_loss:0.056 | g_loss:-0.462\n",
      "iteration:2192 | d_loss:0.055 | g_loss:-0.461\n",
      "iteration:2193 | d_loss:0.056 | g_loss:-0.462\n",
      "iteration:2194 | d_loss:0.056 | g_loss:-0.462\n",
      "iteration:2195 | d_loss:0.055 | g_loss:-0.462\n",
      "iteration:2196 | d_loss:0.056 | g_loss:-0.462\n",
      "iteration:2197 | d_loss:0.056 | g_loss:-0.462\n",
      "iteration:2198 | d_loss:0.056 | g_loss:-0.463\n",
      "iteration:2199 | d_loss:0.056 | g_loss:-0.463\n",
      "iteration:2200 | d_loss:0.056 | g_loss:-0.464\n",
      "iteration:2201 | d_loss:0.056 | g_loss:-0.464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2202 | d_loss:0.057 | g_loss:-0.464\n",
      "iteration:2203 | d_loss:0.056 | g_loss:-0.465\n",
      "iteration:2204 | d_loss:0.056 | g_loss:-0.465\n",
      "iteration:2205 | d_loss:0.058 | g_loss:-0.465\n",
      "iteration:2206 | d_loss:0.057 | g_loss:-0.466\n",
      "iteration:2207 | d_loss:0.058 | g_loss:-0.466\n",
      "iteration:2208 | d_loss:0.057 | g_loss:-0.467\n",
      "iteration:2209 | d_loss:0.057 | g_loss:-0.467\n",
      "iteration:2210 | d_loss:0.058 | g_loss:-0.467\n",
      "iteration:2211 | d_loss:0.058 | g_loss:-0.468\n",
      "iteration:2212 | d_loss:0.057 | g_loss:-0.468\n",
      "iteration:2213 | d_loss:0.058 | g_loss:-0.468\n",
      "iteration:2214 | d_loss:0.058 | g_loss:-0.468\n",
      "iteration:2215 | d_loss:0.058 | g_loss:-0.469\n",
      "iteration:2216 | d_loss:0.058 | g_loss:-0.469\n",
      "iteration:2217 | d_loss:0.057 | g_loss:-0.470\n",
      "iteration:2218 | d_loss:0.058 | g_loss:-0.470\n",
      "iteration:2219 | d_loss:0.059 | g_loss:-0.470\n",
      "iteration:2220 | d_loss:0.058 | g_loss:-0.471\n",
      "iteration:2221 | d_loss:0.059 | g_loss:-0.471\n",
      "iteration:2222 | d_loss:0.058 | g_loss:-0.472\n",
      "iteration:2223 | d_loss:0.059 | g_loss:-0.472\n",
      "iteration:2224 | d_loss:0.059 | g_loss:-0.472\n",
      "iteration:2225 | d_loss:0.059 | g_loss:-0.472\n",
      "iteration:2226 | d_loss:0.058 | g_loss:-0.473\n",
      "iteration:2227 | d_loss:0.059 | g_loss:-0.473\n",
      "iteration:2228 | d_loss:0.059 | g_loss:-0.474\n",
      "iteration:2229 | d_loss:0.059 | g_loss:-0.474\n",
      "iteration:2230 | d_loss:0.059 | g_loss:-0.475\n",
      "iteration:2231 | d_loss:0.059 | g_loss:-0.475\n",
      "iteration:2232 | d_loss:0.059 | g_loss:-0.476\n",
      "iteration:2233 | d_loss:0.059 | g_loss:-0.476\n",
      "iteration:2234 | d_loss:0.060 | g_loss:-0.476\n",
      "iteration:2235 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:2236 | d_loss:0.059 | g_loss:-0.477\n",
      "iteration:2237 | d_loss:0.060 | g_loss:-0.478\n",
      "iteration:2238 | d_loss:0.059 | g_loss:-0.478\n",
      "iteration:2239 | d_loss:0.060 | g_loss:-0.479\n",
      "iteration:2240 | d_loss:0.059 | g_loss:-0.479\n",
      "iteration:2241 | d_loss:0.060 | g_loss:-0.479\n",
      "iteration:2242 | d_loss:0.059 | g_loss:-0.480\n",
      "iteration:2243 | d_loss:0.059 | g_loss:-0.481\n",
      "iteration:2244 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:2245 | d_loss:0.060 | g_loss:-0.481\n",
      "iteration:2246 | d_loss:0.061 | g_loss:-0.482\n",
      "iteration:2247 | d_loss:0.059 | g_loss:-0.482\n",
      "iteration:2248 | d_loss:0.060 | g_loss:-0.483\n",
      "iteration:2249 | d_loss:0.061 | g_loss:-0.483\n",
      "iteration:2250 | d_loss:0.060 | g_loss:-0.483\n",
      "iteration:2251 | d_loss:0.060 | g_loss:-0.484\n",
      "iteration:2252 | d_loss:0.061 | g_loss:-0.484\n",
      "iteration:2253 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:2254 | d_loss:0.061 | g_loss:-0.485\n",
      "iteration:2255 | d_loss:0.062 | g_loss:-0.486\n",
      "iteration:2256 | d_loss:0.061 | g_loss:-0.486\n",
      "iteration:2257 | d_loss:0.061 | g_loss:-0.487\n",
      "iteration:2258 | d_loss:0.062 | g_loss:-0.487\n",
      "iteration:2259 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:2260 | d_loss:0.062 | g_loss:-0.488\n",
      "iteration:2261 | d_loss:0.062 | g_loss:-0.489\n",
      "iteration:2262 | d_loss:0.062 | g_loss:-0.490\n",
      "iteration:2263 | d_loss:0.063 | g_loss:-0.490\n",
      "iteration:2264 | d_loss:0.062 | g_loss:-0.491\n",
      "iteration:2265 | d_loss:0.064 | g_loss:-0.492\n",
      "iteration:2266 | d_loss:0.064 | g_loss:-0.492\n",
      "iteration:2267 | d_loss:0.064 | g_loss:-0.492\n",
      "iteration:2268 | d_loss:0.063 | g_loss:-0.493\n",
      "iteration:2269 | d_loss:0.064 | g_loss:-0.494\n",
      "iteration:2270 | d_loss:0.064 | g_loss:-0.494\n",
      "iteration:2271 | d_loss:0.065 | g_loss:-0.494\n",
      "iteration:2272 | d_loss:0.065 | g_loss:-0.494\n",
      "iteration:2273 | d_loss:0.065 | g_loss:-0.495\n",
      "iteration:2274 | d_loss:0.065 | g_loss:-0.495\n",
      "iteration:2275 | d_loss:0.065 | g_loss:-0.496\n",
      "iteration:2276 | d_loss:0.066 | g_loss:-0.496\n",
      "iteration:2277 | d_loss:0.066 | g_loss:-0.496\n",
      "iteration:2278 | d_loss:0.066 | g_loss:-0.497\n",
      "iteration:2279 | d_loss:0.067 | g_loss:-0.497\n",
      "iteration:2280 | d_loss:0.067 | g_loss:-0.497\n",
      "iteration:2281 | d_loss:0.067 | g_loss:-0.498\n",
      "iteration:2282 | d_loss:0.067 | g_loss:-0.498\n",
      "iteration:2283 | d_loss:0.068 | g_loss:-0.498\n",
      "iteration:2284 | d_loss:0.067 | g_loss:-0.498\n",
      "iteration:2285 | d_loss:0.067 | g_loss:-0.499\n",
      "iteration:2286 | d_loss:0.067 | g_loss:-0.499\n",
      "iteration:2287 | d_loss:0.068 | g_loss:-0.499\n",
      "iteration:2288 | d_loss:0.068 | g_loss:-0.499\n",
      "iteration:2289 | d_loss:0.067 | g_loss:-0.500\n",
      "iteration:2290 | d_loss:0.068 | g_loss:-0.500\n",
      "iteration:2291 | d_loss:0.068 | g_loss:-0.500\n",
      "iteration:2292 | d_loss:0.069 | g_loss:-0.500\n",
      "iteration:2293 | d_loss:0.069 | g_loss:-0.501\n",
      "iteration:2294 | d_loss:0.069 | g_loss:-0.501\n",
      "iteration:2295 | d_loss:0.068 | g_loss:-0.501\n",
      "iteration:2296 | d_loss:0.069 | g_loss:-0.501\n",
      "iteration:2297 | d_loss:0.069 | g_loss:-0.501\n",
      "iteration:2298 | d_loss:0.069 | g_loss:-0.502\n",
      "iteration:2299 | d_loss:0.069 | g_loss:-0.502\n",
      "iteration:2300 | d_loss:0.070 | g_loss:-0.502\n",
      "iteration:2301 | d_loss:0.069 | g_loss:-0.502\n",
      "iteration:2302 | d_loss:0.070 | g_loss:-0.503\n",
      "iteration:2303 | d_loss:0.070 | g_loss:-0.503\n",
      "iteration:2304 | d_loss:0.070 | g_loss:-0.503\n",
      "iteration:2305 | d_loss:0.070 | g_loss:-0.503\n",
      "iteration:2306 | d_loss:0.070 | g_loss:-0.503\n",
      "iteration:2307 | d_loss:0.071 | g_loss:-0.504\n",
      "iteration:2308 | d_loss:0.071 | g_loss:-0.504\n",
      "iteration:2309 | d_loss:0.071 | g_loss:-0.504\n",
      "iteration:2310 | d_loss:0.071 | g_loss:-0.504\n",
      "iteration:2311 | d_loss:0.071 | g_loss:-0.505\n",
      "iteration:2312 | d_loss:0.071 | g_loss:-0.505\n",
      "iteration:2313 | d_loss:0.071 | g_loss:-0.505\n",
      "iteration:2314 | d_loss:0.071 | g_loss:-0.505\n",
      "iteration:2315 | d_loss:0.071 | g_loss:-0.505\n",
      "iteration:2316 | d_loss:0.072 | g_loss:-0.505\n",
      "iteration:2317 | d_loss:0.072 | g_loss:-0.506\n",
      "iteration:2318 | d_loss:0.072 | g_loss:-0.506\n",
      "iteration:2319 | d_loss:0.072 | g_loss:-0.506\n",
      "iteration:2320 | d_loss:0.071 | g_loss:-0.506\n",
      "iteration:2321 | d_loss:0.072 | g_loss:-0.506\n",
      "iteration:2322 | d_loss:0.072 | g_loss:-0.507\n",
      "iteration:2323 | d_loss:0.072 | g_loss:-0.507\n",
      "iteration:2324 | d_loss:0.071 | g_loss:-0.507\n",
      "iteration:2325 | d_loss:0.073 | g_loss:-0.507\n",
      "iteration:2326 | d_loss:0.072 | g_loss:-0.507\n",
      "iteration:2327 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2328 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2329 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2330 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2331 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2332 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2333 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2334 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2335 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2336 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2337 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2338 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2339 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2340 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2341 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2342 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2343 | d_loss:0.072 | g_loss:-0.508\n",
      "iteration:2344 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2345 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2346 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2347 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2348 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2349 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2350 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2351 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2352 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2353 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2354 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2355 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2356 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2357 | d_loss:0.070 | g_loss:-0.508\n",
      "iteration:2358 | d_loss:0.071 | g_loss:-0.508\n",
      "iteration:2359 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2360 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2361 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2362 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2363 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2364 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2365 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2366 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2367 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2368 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2369 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2370 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2371 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2372 | d_loss:0.069 | g_loss:-0.508\n",
      "iteration:2373 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2374 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2375 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2376 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2377 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2378 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2379 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2380 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2381 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2382 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2383 | d_loss:0.068 | g_loss:-0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2384 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2385 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2386 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2387 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2388 | d_loss:0.068 | g_loss:-0.508\n",
      "iteration:2389 | d_loss:0.068 | g_loss:-0.509\n",
      "iteration:2390 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2391 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2392 | d_loss:0.067 | g_loss:-0.508\n",
      "iteration:2393 | d_loss:0.068 | g_loss:-0.509\n",
      "iteration:2394 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2395 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2396 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2397 | d_loss:0.068 | g_loss:-0.509\n",
      "iteration:2398 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2399 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2400 | d_loss:0.068 | g_loss:-0.509\n",
      "iteration:2401 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2402 | d_loss:0.066 | g_loss:-0.509\n",
      "iteration:2403 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2404 | d_loss:0.066 | g_loss:-0.509\n",
      "iteration:2405 | d_loss:0.066 | g_loss:-0.509\n",
      "iteration:2406 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2407 | d_loss:0.067 | g_loss:-0.510\n",
      "iteration:2408 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2409 | d_loss:0.067 | g_loss:-0.510\n",
      "iteration:2410 | d_loss:0.067 | g_loss:-0.509\n",
      "iteration:2411 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2412 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2413 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2414 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2415 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2416 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2417 | d_loss:0.067 | g_loss:-0.510\n",
      "iteration:2418 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2419 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2420 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2421 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2422 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2423 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2424 | d_loss:0.064 | g_loss:-0.510\n",
      "iteration:2425 | d_loss:0.066 | g_loss:-0.510\n",
      "iteration:2426 | d_loss:0.065 | g_loss:-0.511\n",
      "iteration:2427 | d_loss:0.066 | g_loss:-0.511\n",
      "iteration:2428 | d_loss:0.066 | g_loss:-0.511\n",
      "iteration:2429 | d_loss:0.065 | g_loss:-0.511\n",
      "iteration:2430 | d_loss:0.064 | g_loss:-0.510\n",
      "iteration:2431 | d_loss:0.065 | g_loss:-0.511\n",
      "iteration:2432 | d_loss:0.064 | g_loss:-0.511\n",
      "iteration:2433 | d_loss:0.064 | g_loss:-0.511\n",
      "iteration:2434 | d_loss:0.065 | g_loss:-0.511\n",
      "iteration:2435 | d_loss:0.064 | g_loss:-0.511\n",
      "iteration:2436 | d_loss:0.065 | g_loss:-0.511\n",
      "iteration:2437 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2438 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2439 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2440 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2441 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2442 | d_loss:0.064 | g_loss:-0.512\n",
      "iteration:2443 | d_loss:0.064 | g_loss:-0.513\n",
      "iteration:2444 | d_loss:0.064 | g_loss:-0.513\n",
      "iteration:2445 | d_loss:0.064 | g_loss:-0.513\n",
      "iteration:2446 | d_loss:0.063 | g_loss:-0.513\n",
      "iteration:2447 | d_loss:0.063 | g_loss:-0.513\n",
      "iteration:2448 | d_loss:0.063 | g_loss:-0.514\n",
      "iteration:2449 | d_loss:0.064 | g_loss:-0.514\n",
      "iteration:2450 | d_loss:0.063 | g_loss:-0.514\n",
      "iteration:2451 | d_loss:0.063 | g_loss:-0.514\n",
      "iteration:2452 | d_loss:0.063 | g_loss:-0.514\n",
      "iteration:2453 | d_loss:0.064 | g_loss:-0.514\n",
      "iteration:2454 | d_loss:0.063 | g_loss:-0.515\n",
      "iteration:2455 | d_loss:0.062 | g_loss:-0.515\n",
      "iteration:2456 | d_loss:0.063 | g_loss:-0.515\n",
      "iteration:2457 | d_loss:0.063 | g_loss:-0.515\n",
      "iteration:2458 | d_loss:0.063 | g_loss:-0.516\n",
      "iteration:2459 | d_loss:0.062 | g_loss:-0.516\n",
      "iteration:2460 | d_loss:0.062 | g_loss:-0.516\n",
      "iteration:2461 | d_loss:0.064 | g_loss:-0.516\n",
      "iteration:2462 | d_loss:0.063 | g_loss:-0.517\n",
      "iteration:2463 | d_loss:0.063 | g_loss:-0.517\n",
      "iteration:2464 | d_loss:0.063 | g_loss:-0.517\n",
      "iteration:2465 | d_loss:0.063 | g_loss:-0.517\n",
      "iteration:2466 | d_loss:0.064 | g_loss:-0.518\n",
      "iteration:2467 | d_loss:0.063 | g_loss:-0.518\n",
      "iteration:2468 | d_loss:0.062 | g_loss:-0.518\n",
      "iteration:2469 | d_loss:0.065 | g_loss:-0.519\n",
      "iteration:2470 | d_loss:0.062 | g_loss:-0.518\n",
      "iteration:2471 | d_loss:0.062 | g_loss:-0.519\n",
      "iteration:2472 | d_loss:0.062 | g_loss:-0.519\n",
      "iteration:2473 | d_loss:0.064 | g_loss:-0.519\n",
      "iteration:2474 | d_loss:0.062 | g_loss:-0.519\n",
      "iteration:2475 | d_loss:0.063 | g_loss:-0.519\n",
      "iteration:2476 | d_loss:0.063 | g_loss:-0.520\n",
      "iteration:2477 | d_loss:0.063 | g_loss:-0.520\n",
      "iteration:2478 | d_loss:0.062 | g_loss:-0.520\n",
      "iteration:2479 | d_loss:0.063 | g_loss:-0.520\n",
      "iteration:2480 | d_loss:0.063 | g_loss:-0.521\n",
      "iteration:2481 | d_loss:0.063 | g_loss:-0.521\n",
      "iteration:2482 | d_loss:0.063 | g_loss:-0.521\n",
      "iteration:2483 | d_loss:0.063 | g_loss:-0.521\n",
      "iteration:2484 | d_loss:0.063 | g_loss:-0.521\n",
      "iteration:2485 | d_loss:0.062 | g_loss:-0.522\n",
      "iteration:2486 | d_loss:0.063 | g_loss:-0.522\n",
      "iteration:2487 | d_loss:0.063 | g_loss:-0.522\n",
      "iteration:2488 | d_loss:0.062 | g_loss:-0.522\n",
      "iteration:2489 | d_loss:0.063 | g_loss:-0.522\n",
      "iteration:2490 | d_loss:0.062 | g_loss:-0.523\n",
      "iteration:2491 | d_loss:0.061 | g_loss:-0.523\n",
      "iteration:2492 | d_loss:0.062 | g_loss:-0.523\n",
      "iteration:2493 | d_loss:0.062 | g_loss:-0.523\n",
      "iteration:2494 | d_loss:0.062 | g_loss:-0.524\n",
      "iteration:2495 | d_loss:0.062 | g_loss:-0.523\n",
      "iteration:2496 | d_loss:0.062 | g_loss:-0.524\n",
      "iteration:2497 | d_loss:0.062 | g_loss:-0.524\n",
      "iteration:2498 | d_loss:0.063 | g_loss:-0.524\n",
      "iteration:2499 | d_loss:0.063 | g_loss:-0.524\n",
      "iteration:2500 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2501 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2502 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2503 | d_loss:0.061 | g_loss:-0.525\n",
      "iteration:2504 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2505 | d_loss:0.061 | g_loss:-0.526\n",
      "iteration:2506 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2507 | d_loss:0.062 | g_loss:-0.525\n",
      "iteration:2508 | d_loss:0.062 | g_loss:-0.526\n",
      "iteration:2509 | d_loss:0.062 | g_loss:-0.526\n",
      "iteration:2510 | d_loss:0.061 | g_loss:-0.526\n",
      "iteration:2511 | d_loss:0.060 | g_loss:-0.526\n",
      "iteration:2512 | d_loss:0.062 | g_loss:-0.526\n",
      "iteration:2513 | d_loss:0.062 | g_loss:-0.526\n",
      "iteration:2514 | d_loss:0.062 | g_loss:-0.527\n",
      "iteration:2515 | d_loss:0.061 | g_loss:-0.526\n",
      "iteration:2516 | d_loss:0.061 | g_loss:-0.527\n",
      "iteration:2517 | d_loss:0.062 | g_loss:-0.527\n",
      "iteration:2518 | d_loss:0.062 | g_loss:-0.527\n",
      "iteration:2519 | d_loss:0.061 | g_loss:-0.527\n",
      "iteration:2520 | d_loss:0.060 | g_loss:-0.528\n",
      "iteration:2521 | d_loss:0.062 | g_loss:-0.528\n",
      "iteration:2522 | d_loss:0.060 | g_loss:-0.528\n",
      "iteration:2523 | d_loss:0.060 | g_loss:-0.528\n",
      "iteration:2524 | d_loss:0.061 | g_loss:-0.528\n",
      "iteration:2525 | d_loss:0.061 | g_loss:-0.528\n",
      "iteration:2526 | d_loss:0.059 | g_loss:-0.528\n",
      "iteration:2527 | d_loss:0.061 | g_loss:-0.529\n",
      "iteration:2528 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2529 | d_loss:0.061 | g_loss:-0.529\n",
      "iteration:2530 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2531 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2532 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2533 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2534 | d_loss:0.060 | g_loss:-0.529\n",
      "iteration:2535 | d_loss:0.060 | g_loss:-0.530\n",
      "iteration:2536 | d_loss:0.061 | g_loss:-0.530\n",
      "iteration:2537 | d_loss:0.060 | g_loss:-0.530\n",
      "iteration:2538 | d_loss:0.059 | g_loss:-0.530\n",
      "iteration:2539 | d_loss:0.059 | g_loss:-0.530\n",
      "iteration:2540 | d_loss:0.061 | g_loss:-0.530\n",
      "iteration:2541 | d_loss:0.060 | g_loss:-0.531\n",
      "iteration:2542 | d_loss:0.058 | g_loss:-0.530\n",
      "iteration:2543 | d_loss:0.059 | g_loss:-0.531\n",
      "iteration:2544 | d_loss:0.058 | g_loss:-0.531\n",
      "iteration:2545 | d_loss:0.059 | g_loss:-0.531\n",
      "iteration:2546 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2547 | d_loss:0.059 | g_loss:-0.531\n",
      "iteration:2548 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2549 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2550 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2551 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2552 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2553 | d_loss:0.058 | g_loss:-0.532\n",
      "iteration:2554 | d_loss:0.059 | g_loss:-0.533\n",
      "iteration:2555 | d_loss:0.058 | g_loss:-0.533\n",
      "iteration:2556 | d_loss:0.058 | g_loss:-0.533\n",
      "iteration:2557 | d_loss:0.058 | g_loss:-0.533\n",
      "iteration:2558 | d_loss:0.057 | g_loss:-0.533\n",
      "iteration:2559 | d_loss:0.059 | g_loss:-0.533\n",
      "iteration:2560 | d_loss:0.057 | g_loss:-0.533\n",
      "iteration:2561 | d_loss:0.057 | g_loss:-0.533\n",
      "iteration:2562 | d_loss:0.057 | g_loss:-0.533\n",
      "iteration:2563 | d_loss:0.056 | g_loss:-0.534\n",
      "iteration:2564 | d_loss:0.056 | g_loss:-0.534\n",
      "iteration:2565 | d_loss:0.056 | g_loss:-0.534\n",
      "iteration:2566 | d_loss:0.058 | g_loss:-0.534\n",
      "iteration:2567 | d_loss:0.057 | g_loss:-0.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2568 | d_loss:0.056 | g_loss:-0.534\n",
      "iteration:2569 | d_loss:0.057 | g_loss:-0.535\n",
      "iteration:2570 | d_loss:0.057 | g_loss:-0.535\n",
      "iteration:2571 | d_loss:0.058 | g_loss:-0.535\n",
      "iteration:2572 | d_loss:0.058 | g_loss:-0.535\n",
      "iteration:2573 | d_loss:0.057 | g_loss:-0.535\n",
      "iteration:2574 | d_loss:0.057 | g_loss:-0.536\n",
      "iteration:2575 | d_loss:0.055 | g_loss:-0.536\n",
      "iteration:2576 | d_loss:0.055 | g_loss:-0.535\n",
      "iteration:2577 | d_loss:0.058 | g_loss:-0.536\n",
      "iteration:2578 | d_loss:0.056 | g_loss:-0.536\n",
      "iteration:2579 | d_loss:0.056 | g_loss:-0.536\n",
      "iteration:2580 | d_loss:0.057 | g_loss:-0.537\n",
      "iteration:2581 | d_loss:0.056 | g_loss:-0.536\n",
      "iteration:2582 | d_loss:0.056 | g_loss:-0.536\n",
      "iteration:2583 | d_loss:0.057 | g_loss:-0.537\n",
      "iteration:2584 | d_loss:0.056 | g_loss:-0.537\n",
      "iteration:2585 | d_loss:0.055 | g_loss:-0.537\n",
      "iteration:2586 | d_loss:0.056 | g_loss:-0.537\n",
      "iteration:2587 | d_loss:0.056 | g_loss:-0.537\n",
      "iteration:2588 | d_loss:0.055 | g_loss:-0.537\n",
      "iteration:2589 | d_loss:0.055 | g_loss:-0.538\n",
      "iteration:2590 | d_loss:0.055 | g_loss:-0.538\n",
      "iteration:2591 | d_loss:0.056 | g_loss:-0.538\n",
      "iteration:2592 | d_loss:0.055 | g_loss:-0.538\n",
      "iteration:2593 | d_loss:0.056 | g_loss:-0.539\n",
      "iteration:2594 | d_loss:0.056 | g_loss:-0.538\n",
      "iteration:2595 | d_loss:0.055 | g_loss:-0.538\n",
      "iteration:2596 | d_loss:0.056 | g_loss:-0.539\n",
      "iteration:2597 | d_loss:0.055 | g_loss:-0.539\n",
      "iteration:2598 | d_loss:0.055 | g_loss:-0.539\n",
      "iteration:2599 | d_loss:0.055 | g_loss:-0.539\n",
      "iteration:2600 | d_loss:0.054 | g_loss:-0.539\n",
      "iteration:2601 | d_loss:0.055 | g_loss:-0.539\n",
      "iteration:2602 | d_loss:0.054 | g_loss:-0.539\n",
      "iteration:2603 | d_loss:0.053 | g_loss:-0.539\n",
      "iteration:2604 | d_loss:0.054 | g_loss:-0.539\n",
      "iteration:2605 | d_loss:0.054 | g_loss:-0.539\n",
      "iteration:2606 | d_loss:0.054 | g_loss:-0.539\n",
      "iteration:2607 | d_loss:0.054 | g_loss:-0.540\n",
      "iteration:2608 | d_loss:0.054 | g_loss:-0.540\n",
      "iteration:2609 | d_loss:0.053 | g_loss:-0.540\n",
      "iteration:2610 | d_loss:0.055 | g_loss:-0.540\n",
      "iteration:2611 | d_loss:0.054 | g_loss:-0.540\n",
      "iteration:2612 | d_loss:0.054 | g_loss:-0.540\n",
      "iteration:2613 | d_loss:0.054 | g_loss:-0.540\n",
      "iteration:2614 | d_loss:0.053 | g_loss:-0.541\n",
      "iteration:2615 | d_loss:0.054 | g_loss:-0.541\n",
      "iteration:2616 | d_loss:0.053 | g_loss:-0.541\n",
      "iteration:2617 | d_loss:0.055 | g_loss:-0.541\n",
      "iteration:2618 | d_loss:0.054 | g_loss:-0.541\n",
      "iteration:2619 | d_loss:0.055 | g_loss:-0.541\n",
      "iteration:2620 | d_loss:0.053 | g_loss:-0.542\n",
      "iteration:2621 | d_loss:0.054 | g_loss:-0.542\n",
      "iteration:2622 | d_loss:0.055 | g_loss:-0.542\n",
      "iteration:2623 | d_loss:0.053 | g_loss:-0.542\n",
      "iteration:2624 | d_loss:0.053 | g_loss:-0.543\n",
      "iteration:2625 | d_loss:0.054 | g_loss:-0.543\n",
      "iteration:2626 | d_loss:0.053 | g_loss:-0.543\n",
      "iteration:2627 | d_loss:0.054 | g_loss:-0.543\n",
      "iteration:2628 | d_loss:0.054 | g_loss:-0.544\n",
      "iteration:2629 | d_loss:0.054 | g_loss:-0.544\n",
      "iteration:2630 | d_loss:0.054 | g_loss:-0.544\n",
      "iteration:2631 | d_loss:0.055 | g_loss:-0.545\n",
      "iteration:2632 | d_loss:0.055 | g_loss:-0.545\n",
      "iteration:2633 | d_loss:0.056 | g_loss:-0.545\n",
      "iteration:2634 | d_loss:0.055 | g_loss:-0.546\n",
      "iteration:2635 | d_loss:0.056 | g_loss:-0.546\n",
      "iteration:2636 | d_loss:0.056 | g_loss:-0.546\n",
      "iteration:2637 | d_loss:0.056 | g_loss:-0.547\n",
      "iteration:2638 | d_loss:0.057 | g_loss:-0.547\n",
      "iteration:2639 | d_loss:0.056 | g_loss:-0.548\n",
      "iteration:2640 | d_loss:0.057 | g_loss:-0.548\n",
      "iteration:2641 | d_loss:0.057 | g_loss:-0.549\n",
      "iteration:2642 | d_loss:0.058 | g_loss:-0.549\n",
      "iteration:2643 | d_loss:0.058 | g_loss:-0.549\n",
      "iteration:2644 | d_loss:0.058 | g_loss:-0.550\n",
      "iteration:2645 | d_loss:0.058 | g_loss:-0.550\n",
      "iteration:2646 | d_loss:0.057 | g_loss:-0.551\n",
      "iteration:2647 | d_loss:0.058 | g_loss:-0.551\n",
      "iteration:2648 | d_loss:0.059 | g_loss:-0.551\n",
      "iteration:2649 | d_loss:0.058 | g_loss:-0.552\n",
      "iteration:2650 | d_loss:0.060 | g_loss:-0.552\n",
      "iteration:2651 | d_loss:0.060 | g_loss:-0.553\n",
      "iteration:2652 | d_loss:0.059 | g_loss:-0.553\n",
      "iteration:2653 | d_loss:0.059 | g_loss:-0.553\n",
      "iteration:2654 | d_loss:0.060 | g_loss:-0.554\n",
      "iteration:2655 | d_loss:0.060 | g_loss:-0.554\n",
      "iteration:2656 | d_loss:0.061 | g_loss:-0.554\n",
      "iteration:2657 | d_loss:0.061 | g_loss:-0.555\n",
      "iteration:2658 | d_loss:0.060 | g_loss:-0.555\n",
      "iteration:2659 | d_loss:0.061 | g_loss:-0.555\n",
      "iteration:2660 | d_loss:0.061 | g_loss:-0.556\n",
      "iteration:2661 | d_loss:0.061 | g_loss:-0.556\n",
      "iteration:2662 | d_loss:0.061 | g_loss:-0.556\n",
      "iteration:2663 | d_loss:0.062 | g_loss:-0.557\n",
      "iteration:2664 | d_loss:0.063 | g_loss:-0.557\n",
      "iteration:2665 | d_loss:0.062 | g_loss:-0.557\n",
      "iteration:2666 | d_loss:0.062 | g_loss:-0.558\n",
      "iteration:2667 | d_loss:0.063 | g_loss:-0.558\n",
      "iteration:2668 | d_loss:0.062 | g_loss:-0.558\n",
      "iteration:2669 | d_loss:0.063 | g_loss:-0.559\n",
      "iteration:2670 | d_loss:0.063 | g_loss:-0.559\n",
      "iteration:2671 | d_loss:0.064 | g_loss:-0.559\n",
      "iteration:2672 | d_loss:0.063 | g_loss:-0.560\n",
      "iteration:2673 | d_loss:0.063 | g_loss:-0.560\n",
      "iteration:2674 | d_loss:0.064 | g_loss:-0.560\n",
      "iteration:2675 | d_loss:0.065 | g_loss:-0.561\n",
      "iteration:2676 | d_loss:0.064 | g_loss:-0.561\n",
      "iteration:2677 | d_loss:0.063 | g_loss:-0.561\n",
      "iteration:2678 | d_loss:0.065 | g_loss:-0.562\n",
      "iteration:2679 | d_loss:0.064 | g_loss:-0.562\n",
      "iteration:2680 | d_loss:0.064 | g_loss:-0.562\n",
      "iteration:2681 | d_loss:0.066 | g_loss:-0.562\n",
      "iteration:2682 | d_loss:0.065 | g_loss:-0.562\n",
      "iteration:2683 | d_loss:0.066 | g_loss:-0.562\n",
      "iteration:2684 | d_loss:0.066 | g_loss:-0.563\n",
      "iteration:2685 | d_loss:0.066 | g_loss:-0.563\n",
      "iteration:2686 | d_loss:0.066 | g_loss:-0.564\n",
      "iteration:2687 | d_loss:0.066 | g_loss:-0.564\n",
      "iteration:2688 | d_loss:0.066 | g_loss:-0.564\n",
      "iteration:2689 | d_loss:0.067 | g_loss:-0.564\n",
      "iteration:2690 | d_loss:0.067 | g_loss:-0.564\n",
      "iteration:2691 | d_loss:0.067 | g_loss:-0.565\n",
      "iteration:2692 | d_loss:0.069 | g_loss:-0.565\n",
      "iteration:2693 | d_loss:0.068 | g_loss:-0.565\n",
      "iteration:2694 | d_loss:0.067 | g_loss:-0.565\n",
      "iteration:2695 | d_loss:0.067 | g_loss:-0.566\n",
      "iteration:2696 | d_loss:0.068 | g_loss:-0.566\n",
      "iteration:2697 | d_loss:0.068 | g_loss:-0.566\n",
      "iteration:2698 | d_loss:0.069 | g_loss:-0.567\n",
      "iteration:2699 | d_loss:0.067 | g_loss:-0.567\n",
      "iteration:2700 | d_loss:0.068 | g_loss:-0.567\n",
      "iteration:2701 | d_loss:0.069 | g_loss:-0.567\n",
      "iteration:2702 | d_loss:0.069 | g_loss:-0.568\n",
      "iteration:2703 | d_loss:0.070 | g_loss:-0.568\n",
      "iteration:2704 | d_loss:0.068 | g_loss:-0.568\n",
      "iteration:2705 | d_loss:0.068 | g_loss:-0.568\n",
      "iteration:2706 | d_loss:0.070 | g_loss:-0.568\n",
      "iteration:2707 | d_loss:0.068 | g_loss:-0.569\n",
      "iteration:2708 | d_loss:0.070 | g_loss:-0.569\n",
      "iteration:2709 | d_loss:0.069 | g_loss:-0.569\n",
      "iteration:2710 | d_loss:0.070 | g_loss:-0.569\n",
      "iteration:2711 | d_loss:0.071 | g_loss:-0.570\n",
      "iteration:2712 | d_loss:0.071 | g_loss:-0.570\n",
      "iteration:2713 | d_loss:0.070 | g_loss:-0.570\n",
      "iteration:2714 | d_loss:0.069 | g_loss:-0.570\n",
      "iteration:2715 | d_loss:0.071 | g_loss:-0.571\n",
      "iteration:2716 | d_loss:0.070 | g_loss:-0.571\n",
      "iteration:2717 | d_loss:0.070 | g_loss:-0.571\n",
      "iteration:2718 | d_loss:0.071 | g_loss:-0.571\n",
      "iteration:2719 | d_loss:0.071 | g_loss:-0.571\n",
      "iteration:2720 | d_loss:0.072 | g_loss:-0.571\n",
      "iteration:2721 | d_loss:0.070 | g_loss:-0.572\n",
      "iteration:2722 | d_loss:0.071 | g_loss:-0.571\n",
      "iteration:2723 | d_loss:0.070 | g_loss:-0.572\n",
      "iteration:2724 | d_loss:0.072 | g_loss:-0.572\n",
      "iteration:2725 | d_loss:0.073 | g_loss:-0.572\n",
      "iteration:2726 | d_loss:0.071 | g_loss:-0.572\n",
      "iteration:2727 | d_loss:0.072 | g_loss:-0.572\n",
      "iteration:2728 | d_loss:0.073 | g_loss:-0.572\n",
      "iteration:2729 | d_loss:0.071 | g_loss:-0.572\n",
      "iteration:2730 | d_loss:0.072 | g_loss:-0.573\n",
      "iteration:2731 | d_loss:0.072 | g_loss:-0.573\n",
      "iteration:2732 | d_loss:0.072 | g_loss:-0.573\n",
      "iteration:2733 | d_loss:0.074 | g_loss:-0.574\n",
      "iteration:2734 | d_loss:0.073 | g_loss:-0.573\n",
      "iteration:2735 | d_loss:0.073 | g_loss:-0.573\n",
      "iteration:2736 | d_loss:0.072 | g_loss:-0.573\n",
      "iteration:2737 | d_loss:0.073 | g_loss:-0.574\n",
      "iteration:2738 | d_loss:0.073 | g_loss:-0.574\n",
      "iteration:2739 | d_loss:0.073 | g_loss:-0.574\n",
      "iteration:2740 | d_loss:0.074 | g_loss:-0.574\n",
      "iteration:2741 | d_loss:0.074 | g_loss:-0.575\n",
      "iteration:2742 | d_loss:0.074 | g_loss:-0.575\n",
      "iteration:2743 | d_loss:0.074 | g_loss:-0.575\n",
      "iteration:2744 | d_loss:0.074 | g_loss:-0.575\n",
      "iteration:2745 | d_loss:0.074 | g_loss:-0.575\n",
      "iteration:2746 | d_loss:0.073 | g_loss:-0.576\n",
      "iteration:2747 | d_loss:0.072 | g_loss:-0.576\n",
      "iteration:2748 | d_loss:0.074 | g_loss:-0.576\n",
      "iteration:2749 | d_loss:0.074 | g_loss:-0.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2750 | d_loss:0.075 | g_loss:-0.577\n",
      "iteration:2751 | d_loss:0.074 | g_loss:-0.577\n",
      "iteration:2752 | d_loss:0.074 | g_loss:-0.577\n",
      "iteration:2753 | d_loss:0.075 | g_loss:-0.578\n",
      "iteration:2754 | d_loss:0.075 | g_loss:-0.578\n",
      "iteration:2755 | d_loss:0.075 | g_loss:-0.578\n",
      "iteration:2756 | d_loss:0.075 | g_loss:-0.579\n",
      "iteration:2757 | d_loss:0.076 | g_loss:-0.579\n",
      "iteration:2758 | d_loss:0.077 | g_loss:-0.580\n",
      "iteration:2759 | d_loss:0.076 | g_loss:-0.580\n",
      "iteration:2760 | d_loss:0.075 | g_loss:-0.580\n",
      "iteration:2761 | d_loss:0.076 | g_loss:-0.581\n",
      "iteration:2762 | d_loss:0.076 | g_loss:-0.581\n",
      "iteration:2763 | d_loss:0.075 | g_loss:-0.581\n",
      "iteration:2764 | d_loss:0.077 | g_loss:-0.582\n",
      "iteration:2765 | d_loss:0.076 | g_loss:-0.582\n",
      "iteration:2766 | d_loss:0.076 | g_loss:-0.582\n",
      "iteration:2767 | d_loss:0.077 | g_loss:-0.582\n",
      "iteration:2768 | d_loss:0.075 | g_loss:-0.582\n",
      "iteration:2769 | d_loss:0.076 | g_loss:-0.583\n",
      "iteration:2770 | d_loss:0.077 | g_loss:-0.583\n",
      "iteration:2771 | d_loss:0.077 | g_loss:-0.583\n",
      "iteration:2772 | d_loss:0.076 | g_loss:-0.583\n",
      "iteration:2773 | d_loss:0.076 | g_loss:-0.583\n",
      "iteration:2774 | d_loss:0.075 | g_loss:-0.584\n",
      "iteration:2775 | d_loss:0.078 | g_loss:-0.584\n",
      "iteration:2776 | d_loss:0.077 | g_loss:-0.584\n",
      "iteration:2777 | d_loss:0.076 | g_loss:-0.584\n",
      "iteration:2778 | d_loss:0.077 | g_loss:-0.584\n",
      "iteration:2779 | d_loss:0.076 | g_loss:-0.584\n",
      "iteration:2780 | d_loss:0.076 | g_loss:-0.584\n",
      "iteration:2781 | d_loss:0.076 | g_loss:-0.585\n",
      "iteration:2782 | d_loss:0.076 | g_loss:-0.585\n",
      "iteration:2783 | d_loss:0.077 | g_loss:-0.585\n",
      "iteration:2784 | d_loss:0.075 | g_loss:-0.585\n",
      "iteration:2785 | d_loss:0.076 | g_loss:-0.585\n",
      "iteration:2786 | d_loss:0.075 | g_loss:-0.585\n",
      "iteration:2787 | d_loss:0.076 | g_loss:-0.585\n",
      "iteration:2788 | d_loss:0.075 | g_loss:-0.585\n",
      "iteration:2789 | d_loss:0.076 | g_loss:-0.586\n",
      "iteration:2790 | d_loss:0.075 | g_loss:-0.585\n",
      "iteration:2791 | d_loss:0.075 | g_loss:-0.586\n",
      "iteration:2792 | d_loss:0.076 | g_loss:-0.586\n",
      "iteration:2793 | d_loss:0.075 | g_loss:-0.586\n",
      "iteration:2794 | d_loss:0.076 | g_loss:-0.586\n",
      "iteration:2795 | d_loss:0.075 | g_loss:-0.586\n",
      "iteration:2796 | d_loss:0.075 | g_loss:-0.586\n",
      "iteration:2797 | d_loss:0.075 | g_loss:-0.586\n",
      "iteration:2798 | d_loss:0.074 | g_loss:-0.587\n",
      "iteration:2799 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2800 | d_loss:0.076 | g_loss:-0.587\n",
      "iteration:2801 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2802 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2803 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2804 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2805 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2806 | d_loss:0.075 | g_loss:-0.587\n",
      "iteration:2807 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2808 | d_loss:0.076 | g_loss:-0.588\n",
      "iteration:2809 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2810 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2811 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2812 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2813 | d_loss:0.075 | g_loss:-0.588\n",
      "iteration:2814 | d_loss:0.074 | g_loss:-0.589\n",
      "iteration:2815 | d_loss:0.075 | g_loss:-0.589\n",
      "iteration:2816 | d_loss:0.075 | g_loss:-0.589\n",
      "iteration:2817 | d_loss:0.076 | g_loss:-0.589\n",
      "iteration:2818 | d_loss:0.075 | g_loss:-0.589\n",
      "iteration:2819 | d_loss:0.075 | g_loss:-0.589\n",
      "iteration:2820 | d_loss:0.074 | g_loss:-0.589\n",
      "iteration:2821 | d_loss:0.074 | g_loss:-0.589\n",
      "iteration:2822 | d_loss:0.075 | g_loss:-0.590\n",
      "iteration:2823 | d_loss:0.074 | g_loss:-0.590\n",
      "iteration:2824 | d_loss:0.075 | g_loss:-0.590\n",
      "iteration:2825 | d_loss:0.075 | g_loss:-0.590\n",
      "iteration:2826 | d_loss:0.075 | g_loss:-0.590\n",
      "iteration:2827 | d_loss:0.074 | g_loss:-0.590\n",
      "iteration:2828 | d_loss:0.074 | g_loss:-0.590\n",
      "iteration:2829 | d_loss:0.075 | g_loss:-0.591\n",
      "iteration:2830 | d_loss:0.075 | g_loss:-0.590\n",
      "iteration:2831 | d_loss:0.075 | g_loss:-0.591\n",
      "iteration:2832 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2833 | d_loss:0.075 | g_loss:-0.591\n",
      "iteration:2834 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2835 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2836 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2837 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2838 | d_loss:0.074 | g_loss:-0.591\n",
      "iteration:2839 | d_loss:0.075 | g_loss:-0.592\n",
      "iteration:2840 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2841 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2842 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2843 | d_loss:0.073 | g_loss:-0.592\n",
      "iteration:2844 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2845 | d_loss:0.073 | g_loss:-0.592\n",
      "iteration:2846 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2847 | d_loss:0.074 | g_loss:-0.593\n",
      "iteration:2848 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2849 | d_loss:0.074 | g_loss:-0.592\n",
      "iteration:2850 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2851 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2852 | d_loss:0.074 | g_loss:-0.593\n",
      "iteration:2853 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2854 | d_loss:0.074 | g_loss:-0.593\n",
      "iteration:2855 | d_loss:0.074 | g_loss:-0.593\n",
      "iteration:2856 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2857 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2858 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2859 | d_loss:0.073 | g_loss:-0.593\n",
      "iteration:2860 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2861 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2862 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2863 | d_loss:0.072 | g_loss:-0.594\n",
      "iteration:2864 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2865 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2866 | d_loss:0.072 | g_loss:-0.594\n",
      "iteration:2867 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2868 | d_loss:0.072 | g_loss:-0.594\n",
      "iteration:2869 | d_loss:0.072 | g_loss:-0.594\n",
      "iteration:2870 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2871 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2872 | d_loss:0.073 | g_loss:-0.594\n",
      "iteration:2873 | d_loss:0.072 | g_loss:-0.594\n",
      "iteration:2874 | d_loss:0.073 | g_loss:-0.595\n",
      "iteration:2875 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2876 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2877 | d_loss:0.073 | g_loss:-0.595\n",
      "iteration:2878 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2879 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2880 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2881 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2882 | d_loss:0.071 | g_loss:-0.595\n",
      "iteration:2883 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2884 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2885 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2886 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2887 | d_loss:0.071 | g_loss:-0.595\n",
      "iteration:2888 | d_loss:0.071 | g_loss:-0.595\n",
      "iteration:2889 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2890 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2891 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2892 | d_loss:0.072 | g_loss:-0.595\n",
      "iteration:2893 | d_loss:0.071 | g_loss:-0.595\n",
      "iteration:2894 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2895 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2896 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2897 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2898 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2899 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2900 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2901 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2902 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2903 | d_loss:0.071 | g_loss:-0.596\n",
      "iteration:2904 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2905 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2906 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2907 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2908 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2909 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2910 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2911 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2912 | d_loss:0.072 | g_loss:-0.596\n",
      "iteration:2913 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2914 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2915 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2916 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2917 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2918 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2919 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2920 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2921 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2922 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2923 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2924 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2925 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2926 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2927 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2928 | d_loss:0.072 | g_loss:-0.597\n",
      "iteration:2929 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2930 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2931 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2932 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2933 | d_loss:0.071 | g_loss:-0.597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:2934 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2935 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2936 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2937 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2938 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2939 | d_loss:0.071 | g_loss:-0.597\n",
      "iteration:2940 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2941 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2942 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2943 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2944 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2945 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2946 | d_loss:0.070 | g_loss:-0.597\n",
      "iteration:2947 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2948 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2949 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2950 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2951 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2952 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2953 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2954 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2955 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2956 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2957 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2958 | d_loss:0.070 | g_loss:-0.596\n",
      "iteration:2959 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2960 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2961 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2962 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2963 | d_loss:0.069 | g_loss:-0.595\n",
      "iteration:2964 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2965 | d_loss:0.069 | g_loss:-0.596\n",
      "iteration:2966 | d_loss:0.069 | g_loss:-0.595\n",
      "iteration:2967 | d_loss:0.069 | g_loss:-0.595\n",
      "iteration:2968 | d_loss:0.069 | g_loss:-0.595\n",
      "iteration:2969 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2970 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2971 | d_loss:0.069 | g_loss:-0.595\n",
      "iteration:2972 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2973 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2974 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2975 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2976 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2977 | d_loss:0.068 | g_loss:-0.595\n",
      "iteration:2978 | d_loss:0.068 | g_loss:-0.594\n",
      "iteration:2979 | d_loss:0.068 | g_loss:-0.594\n",
      "iteration:2980 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2981 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2982 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2983 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2984 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2985 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2986 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2987 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2988 | d_loss:0.067 | g_loss:-0.594\n",
      "iteration:2989 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2990 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2991 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2992 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2993 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2994 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2995 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2996 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2997 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2998 | d_loss:0.066 | g_loss:-0.593\n",
      "iteration:2999 | d_loss:0.065 | g_loss:-0.593\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "g_loss_list = []\n",
    "r_loss_list = []\n",
    "f_loss_list = []\n",
    "f_r_loss_list = []\n",
    "penalty_list = []\n",
    "d_loss_list = []\n",
    "batch_size = 336\n",
    "iterations = 3000\n",
    "\n",
    "# Training Data\n",
    "X_train = np.expand_dims(processed_data, axis = 3)\n",
    "\n",
    "# Labels\n",
    "true_label = np.ones((batch_size, 1))\n",
    "fake_label = np.zeros((batch_size, 1))\n",
    "\n",
    "# Training\n",
    "for epoch in range(iterations):\n",
    "    \n",
    "    # Training the Discriminator\n",
    "    # Generator in\n",
    "    z = np.random.normal(0.5, 0.25, (batch_size, X_train.shape[1], X_train.shape[2], 1))\n",
    "    # Generator out Images\n",
    "    f_imgs = generator.predict(z)\n",
    "    # Real Images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    r_imgs = X_train[idx]\n",
    "    # train the discriminator\n",
    "    epsilon = np.random.uniform(size = (batch_size, 1,1,1))\n",
    "    r_loss, f_loss, penalty, d_loss = D_train([r_imgs, z, epsilon])\n",
    "    \n",
    "    # Training the Discriminator\n",
    "    # Generator in\n",
    "    z = np.random.normal(0.5, 0.25, (batch_size, X_train.shape[1], X_train.shape[2], 1))\n",
    "    # train the generator\n",
    "    g_loss = G_train([z])\n",
    "      \n",
    "    #### Record of learning progress\n",
    "    # loss\n",
    "    r_loss_list.append(r_loss)\n",
    "    f_loss_list.append(f_loss)\n",
    "    f_r_loss_list.append(f_loss - r_loss)\n",
    "    penalty_list.append(penalty)\n",
    "    d_loss_list.append(d_loss)\n",
    "    # generated image sumple\n",
    "    print(f'iteration:{epoch} | d_loss:{d_loss:.3f} | g_loss:{sum(g_loss)/len(g_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = np.mean(processed_data, axis = 0)\n",
    "variance_data = np.mean((processed_data - mean_data )* (processed_data- mean_data ), axis = 0)\n",
    "skewness_data = np.mean((processed_data - mean_data )* (processed_data- mean_data ) * (processed_data- mean_data ), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.49547619 0.5014881 ]\n",
      " [0.50002976 0.50017857 0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "------------\n",
      "[[0.02188512 0.00780811 0.00074183]\n",
      " [0.01493125 0.04641009 0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "------------\n",
      "[[0.00249854 0.00381854 0.00036871]\n",
      " [0.00011419 0.00261314 0.        ]\n",
      " [0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_data)\n",
    "print('------------')\n",
    "print(variance_data)\n",
    "print('------------')\n",
    "print(skewness_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics of the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_noise= np.random.normal(0.5, 0.25, (batch_size, X_train.shape[1], X_train.shape[2], 1))\n",
    "gen_data = np.squeeze(generator.predict(gaussian_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gen_data = np.mean(gen_data, axis = 0)\n",
    "variance_gen_data = np.mean((processed_data - mean_gen_data)* (processed_data- mean_gen_data), axis = 0)\n",
    "skewness_gen_data = np.mean((processed_data - mean_gen_data)* (processed_data- mean_gen_data) * (processed_data- mean_gen_data), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0428712  -0.85798156  0.91448504]\n",
      " [ 1.1955582   0.9390683  -0.37148285]\n",
      " [ 1.2076294  -0.13540411  0.15105346]]\n",
      "------------\n",
      "[[0.3165943  1.839656   0.17130831]\n",
      " [0.49869104 0.2390343  0.13799951]\n",
      " [1.45836887 0.01833427 0.02281715]]\n",
      "------------\n",
      "[[-1.93133006e-01  2.51485109e+00 -7.09938480e-02]\n",
      " [-3.67509827e-01 -1.43034387e-01  5.12644501e-02]\n",
      " [-1.76116918e+00  2.48253592e-03 -3.44660901e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_gen_data)\n",
    "print('------------')\n",
    "print(variance_gen_data)\n",
    "print('------------')\n",
    "print(skewness_gen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
